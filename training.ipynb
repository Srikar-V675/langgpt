{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-12T13:16:38.698247Z","iopub.execute_input":"2024-07-12T13:16:38.698949Z","iopub.status.idle":"2024-07-12T13:16:38.703777Z","shell.execute_reply.started":"2024-07-12T13:16:38.698916Z","shell.execute_reply":"2024-07-12T13:16:38.702871Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport math\nfrom pathlib import Path\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport time\nimport torch\nimport random\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler, autocast\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.trainers import BpeTrainer\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:16:38.705611Z","iopub.execute_input":"2024-07-12T13:16:38.705962Z","iopub.status.idle":"2024-07-12T13:16:50.587519Z","shell.execute_reply.started":"2024-07-12T13:16:38.705930Z","shell.execute_reply":"2024-07-12T13:16:50.586532Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"wandb.login(key = '6f39f273bf3a9cafe3034b72701f70a6f35e315d')","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:16:50.589166Z","iopub.execute_input":"2024-07-12T13:16:50.589513Z","iopub.status.idle":"2024-07-12T13:16:52.488831Z","shell.execute_reply.started":"2024-07-12T13:16:50.589485Z","shell.execute_reply":"2024-07-12T13:16:52.487970Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"wandb.init(\n    project=\"LangGPT\",\n    config={\n        \"architecture\": \"Transformers\",\n        \"dataset\": \"https://huggingface.co/datasets/cfilt/iitb-english-hindi\",\n        \"epochs\": 20,\n        \"Training Data\": 50000,\n        \"Version\": \"V1\",\n        \"tags\": \"Kaggle GPU T4 Run2\",\n    },\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-12T13:16:52.489828Z","iopub.execute_input":"2024-07-12T13:16:52.490263Z","iopub.status.idle":"2024-07-12T13:17:10.148902Z","shell.execute_reply.started":"2024-07-12T13:16:52.490221Z","shell.execute_reply":"2024-07-12T13:17:10.147919Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrikar_v\u001b[0m (\u001b[33msrikarv44\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240712_131652-db2dn28c</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/srikarv44/LangGPT/runs/db2dn28c' target=\"_blank\">scarlet-surf-36</a></strong> to <a href='https://wandb.ai/srikarv44/LangGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/srikarv44/LangGPT' target=\"_blank\">https://wandb.ai/srikarv44/LangGPT</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/srikarv44/LangGPT/runs/db2dn28c' target=\"_blank\">https://wandb.ai/srikarv44/LangGPT/runs/db2dn28c</a>"},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/srikarv44/LangGPT/runs/db2dn28c?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7ac299f0a980>"},"metadata":{}}]},{"cell_type":"code","source":"# :: DATASET ::\n# Download dataset from Hugging-face: https://huggingface.co/datasets/cfilt/iitb-english-hindi\nprint(\"INFO: Dataset download started.\")\nraw_train_dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=\"train\")\nraw_val_dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=\"validation\")\nraw_test_dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=\"test\")\nprint(\"INFO: Dataset download complete.\")\n\n# Set seeds for reproducibility\ntorch.manual_seed(69)\nrandom.seed(69)\nnp.random.seed(69)\n\n# # Splitting the dataset into training and validation dataset of 3000 and 300 respectively for faster training and validation.\nraw_train_dataset, rt_to_skip = random_split(raw_train_dataset, [50000, len(raw_train_dataset) - 50000])\nprint(len(raw_train_dataset))\n# raw_val_dataset, vt_to_skip = random_split(raw_val_dataset, [500, len(raw_val_dataset) - 500])","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:17:10.152158Z","iopub.execute_input":"2024-07-12T13:17:10.152474Z","iopub.status.idle":"2024-07-12T13:17:17.916190Z","shell.execute_reply.started":"2024-07-12T13:17:10.152446Z","shell.execute_reply":"2024-07-12T13:17:17.914760Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"INFO: Dataset download started.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/3.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"590bfb1ee6834987897532879512ff36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76c526285ef84fd69899dd6f77760df1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/190M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f78f80482e14fb193387e4a1010b3e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/85.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f9b7fc6793a44e6b94e31e1b69bac61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cb77c1d035042ad8fb048e87f507311"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1659083 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1f1387c82f0463f957e7542a6ec0ac0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/520 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a934f82d6f194170b68ac735da88514e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2507 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb4fd9d74b0146b5a624c9241cda3d3e"}},"metadata":{}},{"name":"stdout","text":"INFO: Dataset download complete.\n50000\n","output_type":"stream"}]},{"cell_type":"code","source":"# :: TOKENIZER :: \n# [ Creating Source Language Tokenizer - English ]\n# Additional Special Tokens: [UNK] - to represent Unknown words, [PAD] - to represent padding added to keep sequence length constant for the model\n# [CLS] - Token to denote start of sentence, [SEP] = Token to denote end of sentence\n\ntokenizer_en = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer_en = BpeTrainer(\n    min_frequency=2, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n)\n\n# NOTE: below function is used as an iterator on the smaller random dataset we just \ndef get_ds_iterator(raw_train_dataset, lang):\n    for data in raw_train_dataset:\n        yield data['translation'][lang]\n    \n\n# splitting tokens based on whitespaces\ntokenizer_en.pre_tokenizer = Whitespace()\nprint(\"INFO: source tokenizer initialized\")\n\nprint(\"INFO: source tokenizer training started...\")\nstart_time = time.time()\ntokenizer_en.train_from_iterator(get_ds_iterator(raw_train_dataset, \"en\"), trainer=trainer_en)\n# tokenizer_en.train(files=path_en, trainer=trainer_en)\nprint(\"INFO: source tokenizer training completed!\")\nprint(f\"INFO: time taken: {time.time() - start_time}s\")\n\n\n# Save tokenizer for future use\ntokenizer_en.save(\"/kaggle/working/tokenizer_en.json\")\nprint(\n    f\"INFO: source tokenizer saved into: /models/tokenizer_en/tokenizer_en.json\"\n)\n\n\n# [ Creating Target Language Tokenizer - Hindi ]\ntokenizer_hi = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer_hi = BpeTrainer(\n    min_frequency=2, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n)\n\n# splitting tokens based on whitespaces\ntokenizer_hi.pre_tokenizer = Whitespace()\nprint(\"INFO: target tokenizer initialized\")\n\nprint(\"INFO: target tokenizer training started...\")\nstart_time = time.time()\ntokenizer_hi.train_from_iterator(get_ds_iterator(raw_train_dataset, \"hi\"), trainer=trainer_hi)\n# tokenizer_hi.train(files=path_hi, trainer=trainer_hi)\nprint(\"INFO: target tokenizer training completed!\")\nprint(f\"INFO: time taken: {time.time() - start_time}s\")\n\n# Save tokenizer for future use\ntokenizer_hi.save(\"/kaggle/working/tokenizer_hi.json\")\nprint(\n    f\"INFO: source tokenizer saved into: /models/tokenizer_hi/tokenizer_hi.json\"\n)\n\n# Load tokenizers from file\ntokenizer_en = Tokenizer.from_file(\"/kaggle/working/tokenizer_en.json\")\ntokenizer_hi = Tokenizer.from_file(\"/kaggle/working/tokenizer_hi.json\")\n\n# Store the vocab size of source and target tokenizers\nsource_vocab_size = tokenizer_en.get_vocab_size()\ntarget_vocab_size = tokenizer_hi.get_vocab_size()\nprint(f\"INFO: source tokenizer vocab size = {source_vocab_size}\")\nprint(f\"INFO: target tokenizer vocab size = {target_vocab_size}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:17:17.918011Z","iopub.execute_input":"2024-07-12T13:17:17.918412Z","iopub.status.idle":"2024-07-12T13:17:27.270571Z","shell.execute_reply.started":"2024-07-12T13:17:17.918353Z","shell.execute_reply":"2024-07-12T13:17:27.269538Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"INFO: source tokenizer initialized\nINFO: source tokenizer training started...\n\n\n\nINFO: source tokenizer training completed!\nINFO: time taken: 4.529663562774658s\nINFO: source tokenizer saved into: /models/tokenizer_en/tokenizer_en.json\nINFO: target tokenizer initialized\nINFO: target tokenizer training started...\n\n\n\nINFO: target tokenizer training completed!\nINFO: time taken: 4.624605417251587s\nINFO: source tokenizer saved into: /models/tokenizer_hi/tokenizer_hi.json\nINFO: source tokenizer vocab size = 30000\nINFO: target tokenizer vocab size = 30000\n","output_type":"stream"}]},{"cell_type":"code","source":"# :: TRAINING ::\nst_time = time.time()\n\n# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"INFO: device = {device}\")\n\nprint(\"INFO: line:23 -> EncodeDataset\")\n# This class takes raw dataset and max_seq_len\nclass EncodeDataset(Dataset):\n    def __init__(self, raw_dataset, max_seq_len):\n        \"\"\"\n        Constructor to initialise class variables\n\n        Args:\n            raw_dataset (Dataset): raw data downloaded from hugging-face\n            max_seq_len (int): max seq length of the sentences in the dataset\n        \"\"\"\n        super().__init__()\n        self.raw_dataset = raw_dataset\n        self.max_seq_len = max_seq_len\n\n    def __len__(self):\n        return len(self.raw_dataset)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Generating below for the translation pair at raw_dataset[index]:\n        - encoder_input\n        - decoder_input\n        - target_label\n        - encoder_mask\n        - decoder_mask\n        - source_text\n        - target_text\n\n        Args:\n            index (_type_): item at index in raw_dataset\n        \"\"\"\n\n        # Fetch the raw translation\n        raw_text = self.raw_dataset[index]\n\n        # Split into source and target text\n        source_text = raw_text['translation']['en']\n        target_text = raw_text['translation']['hi']\n\n        # Encoding source text using source tokenizer(tokenizer_en) and target text using target tokenizer(tokenizer_hi)\n        source_text_encoded = tokenizer_en.encode(source_text).ids\n        target_text_encoded = tokenizer_hi.encode(target_text).ids\n\n        # Convert the CLS, SEP and PAD tokens to their corresponding index id in vocabulary using tokenizer [the id would be same with either tokenizers]\n        CLS_ID = torch.tensor([tokenizer_hi.token_to_id(\"[CLS]\")], dtype=torch.int64)\n        SEP_ID = torch.tensor([tokenizer_hi.token_to_id(\"[SEP]\")], dtype=torch.int64)\n        PAD_ID = torch.tensor([tokenizer_hi.token_to_id(\"[PAD]\")], dtype=torch.int64)\n        \n        # To train the model we have to same sequence length for input and output and hence we need to add padding\n        # Calculate the number of padding to be added for source and target\n        num_source_padding = (\n            self.max_seq_len - len(source_text_encoded) - 2\n        )  # 2 -> [CLS] and [SEP]\n        num_target_padding = (\n            self.max_seq_len - len(target_text_encoded) - 1\n        )  # 1 -> [SEP] only because target label contains [SEP] only and [CLS] is required by the model to start the inference\n\n        # Add the padding based on the number computer above\n        encoder_padding = torch.tensor([PAD_ID] * num_source_padding, dtype=torch.int64)\n        decoder_padding = torch.tensor([PAD_ID] * num_target_padding, dtype=torch.int64)\n\n        # construct the encoder input\n        # Encoder I/P: [CLS_ID] + source_text_encoded + [SEP_ID] + encoder_padding\n        encoder_input = torch.cat([CLS_ID, torch.tensor(source_text_encoded, dtype=torch.int64), SEP_ID, encoder_padding], dim=0)\n\n        # construct the decoder input\n        # Decoder I/P: [CLS_ID] + target_text_encoded + decoder_padding\n        decoder_input = torch.cat([CLS_ID, torch.tensor(target_text_encoded, dtype=torch.int64), decoder_padding ], dim=0)\n\n        # construct the target label\n        # Target Label: target_text_encoded + [SEP_ID] + decoder_padding\n        target_label = torch.cat([torch.tensor(target_text_encoded, dtype=torch.int64), SEP_ID, decoder_padding], dim=0)\n\n        # As we are adding extra padding to match the sequence input,but we should not let the model train on it\n        # hence, we'll use encoder mask to nullify the padding tokens\n        encoder_mask = (\n            (encoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int()\n        )\n\n        # We'll do that same for decoder too but we also need to get rid of the upper triangle for Masked Multi-Attention\n        decoder_mask = (decoder_input != PAD_ID).unsqueeze(0).unsqueeze(\n            0\n        ).int() & causal_mask(decoder_input.size(0))\n\n        return {\n            \"encoder_input\": encoder_input,\n            \"decoder_input\": decoder_input,\n            \"target_label\": target_label,\n            \"encoder_mask\": encoder_mask,\n            \"decoder_mask\": decoder_mask,\n            \"source_text\": source_text,\n            \"target_text\": target_text,\n        }\n\n\n# Causal mask will make sure any token that comes after the current token will be masked, meaning the value will be replaced by -ve infinity which will be converted to zero or close to zero after softmax function.\n# Hence the model will just ignore these value or willn't be able to learn anything from these values.\ndef causal_mask(size):\n    # dimension of causal mask (batch_size, seq_len, seq_len)\n    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n    return mask == 0\n\n\n# # calculating max_seq_len from the dataset\n# max_seq_len_source = 0\n# max_seq_len_target = 0\n\n# for data in raw_train_dataset:\n#     enc_ids = tokenizer_en.encode(data['translation']['en']).ids\n#     dec_ids = tokenizer_hi.encode(data['translation']['hi']).ids\n#     max_seq_len_source = max(max_seq_len_source, len(enc_ids))\n#     max_seq_len_target = max(max_seq_len_target, len(dec_ids))\n\n\n# print(f\"Max sequence length of source: {max_seq_len_source}\")  # 50\n# print(f\"Max sequence length of target: {max_seq_len_target}\")  # 50\n\n\n# To simplify the calcualtion let's add some value to the greater value and have a single max_seq_len\n#max_seq_len = 100  # 50 + 20: 20 -> to accomodate the additional length of tokens such as PAD, CLS, SEP in the sequence.\n# max_seq_len = max(max_seq_len_source, max_seq_len_target) + 20\nmax_seq_len = 548\n\n\nprint(\"INFO: Encoding dataset started.\")\n# Instantiate the EncodeDataset class and create the encoded train and validation-dataset.\ntrain_dataset = EncodeDataset(raw_train_dataset, max_seq_len)\nval_dataset = EncodeDataset(raw_val_dataset, max_seq_len)\nprint(\"INFO: Encoding dataset complete.\")\n\n\n# --------------------------------------------------- #\n# Input Embedding and Positional Encoding\n\nprint(\"INFO: line 152 -> EmbeddingLayer\")\n\n\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int):\n        super().__init__()\n        self.d_model = d_model\n\n        # using pytorch's embedding module we will map the token_id with the vocabulary and then convert it to embedding matrix\n        self.embedding = nn.Embedding(\n            vocab_size, d_model\n        )  # initialise the Embedding layer to taken in the vocab_size and output a embeddign vector of size d_model\n\n    def forward(self, input):\n        # After the output of embedding is recieved the output is multiplied with teh sqrt(d_model) for normalizing the output\n        embedding_output = self.embedding(input) * math.sqrt(self.d_model)\n        return embedding_output\n    \n\nprint(\"INFO: line 169 -> PositionalEncoding\")\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_seq_len: int, dropout_rate: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # we're creating a zero matrix of the same size as the embedding matrix\n        pe = torch.zeros(max_seq_len, d_model)\n\n        # Calculate the position part of the PE function\n        pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n\n        # Calculate the division part of the PE function\n        # NOTE: div part expression is slightly different that papers expression as this exponential functions seems to works better.\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\n        # Fill in the `pe` with the sin and cos of the PE function\n        # NOTE: sin -> even pos\n        # NOTE: cos -> odd pos\n\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n\n        # Since we're expecting the input sequences in batches so the extra batch_size dimension is added in 0 postion.\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe) # LEARN: what is register_buffer\n\n    def forward(self, input_embdding):\n        # Add positional encoding together with the input embedding vector.\n        input_embdding = input_embdding + (\n            self.pe[:, : input_embdding.shape[1], :]\n        ).requires_grad_(False) # to prevent from calculating the gradient of the positional encoding.\n\n        # Perform dropout to prevent overfitting.\n        return self.dropout(input_embdding)\n\n    \n# --------------------------------------------------- #\n# Multi-head Attention Block\n\nprint(\"INFO: line 208 -> MultiHeadAttention\")\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout_rate: float):\n        super().__init__()\n        # Define dropout to prevent overfitting\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # Weight matrices are intoduced and all are learnable params\n        self.W_q = nn.Linear(d_model, d_model, bias=False)  # Linear -> to enable learning # NOTE: bias=False -> to prevent bias newly added\n        self.W_k = nn.Linear(d_model, d_model, bias=False)\n        self.W_v = nn.Linear(d_model, d_model, bias=False)\n        self.W_o = nn.Linear(d_model, d_model, bias=False)\n\n        self.num_heads = num_heads\n        assert d_model % num_heads == 0, \"d_model must be divisible by number of heads\"\n\n        # d_k is the new dimension of each of each splitted self-attention heads\n        self.d_k = d_model // num_heads\n\n    def forward(self, q, k, v, encoder_mask):\n\n        # # We'll be training our model with multiple batches of sequence at once in parallel, hence we'll need to include batch_size in the shape as well.\n        # query, key and value are calculated by matrix multiplication of corresponding weights with the input embeddings.\n        # Change of shape: q(batch_size, seq_len, d_model) @ W_q(d_model, d_model) => query(batch_size, seq_len, d_model) [same goes to key and value].\n        query = self.W_q(q)\n        key = self.W_k(k)\n        value = self.W_v(v)\n\n        # Splitting query, key and value into number of heads. d_model is splitted in d_k across 8 heads.\n        # Change of shape: query(batch_size, seq_len, d_model) => query(batch_size, seq_len, num_heads, d_k) -> query(batch_size,num_heads, seq_len,d_k) [same goes to key and value].\n        query = query.view(query.shape[0], query.shape[1], self.num_heads ,self.d_k).transpose(1,2)\n        key = key.view(key.shape[0], key.shape[1], self.num_heads ,self.d_k).transpose(1,2)\n        value = value.view(value.shape[0], value.shape[1], self.num_heads ,self.d_k).transpose(1,2)\n\n        # INFO: SELF-ATTENTION BLOCK STARTS INFO:\n        \n        with autocast():\n            attention_score = (query @ key.transpose(-2, -1)) / math.sqrt(self.d_k)\n            attention_score = attention_score.float()\n            \n            if encoder_mask is not None:\n                attention_score = attention_score.masked_fill(encoder_mask == 0, -1e9)\n\n            attention_score = F.softmax(attention_score, dim=-1)\n            if self.dropout is not None:\n                attention_score = self.dropout(attention_score)\n            \n            attention_score = attention_score.type_as(query)\n            attention_output = attention_score @ value\n\n#         # Attention score is calculated\n#         # Change of shape: query(batch_size,num_heads, seq_len,d_k) @ key(batch_size,num_heads, seq_len,d_k) => attention_score(batch_size,num_heads, seq_len,seq_len).\n#         attention_score = (query @ key.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n#         # Cast to float32 before applying the mask and softmax\n#         attention_score = attention_score.float()\n        \n#         # If masking is available\n#         if encoder_mask is not None:\n#             attention_score.masked_fill_(encoder_mask==0, -1e9)\n\n#         # Softmax function calculates the probability distribution among all the attention scores. It assign higher probabiliy value to higher attention score. Meaning more similar tokens get higher probability value.\n#         # Change of shape: same as attention_score\n#         # attention_weight = torch.softmax(attention_score, dim=-1)\n#         attention_score = attention_score.softmax(dim=-1)\n\n#         if self.dropout is not None:\n#             attention_score = self.dropout(attention_score)\n        \n#         # Cast back to the original dtype if necessary\n#         #attention_score = attention_score.type_as(query)\n\n#         # Final step in Self attention block is, matrix multiplication of attention_weight with Value embedding vector.\n#         # Change of shape: attention_score(batch_size,num_heads, seq_len,seq_len) @  value(batch_size,num_heads, seq_len,d_k) => attention_output(batch_size,num_heads, seq_len,d_k)\n#         attention_output = attention_score @ value\n\n        # INFO: SELF-ATTENTION BLOCK ENDS\n\n        # Now, all heads must be combined back to a single head\n        # Change of shape:attention_output(batch_size,num_heads, seq_len,d_k) => attention_output(batch_size,seq_len,num_heads,d_k) => attention_output(batch_size,seq_len,d_model)\n        attention_output = attention_output.transpose(1,2).contiguous().view(attention_output.shape[0], -1, self.num_heads * self.d_k)\n\n        # Finally attention_output is matrix multiplied with output weight matrix to give the final Multi-Head attention output.\n        # The shape of the multihead_output is same as the embedding input\n        # Change of shape: attention_output(batch_size,seq_len,d_model) @ W_o(d_model, d_model) => multihead_output(batch_size, seq_len, d_model)\n        multihead_output = self.W_o(attention_output)\n\n        return multihead_output\n\n    \n# --------------------------------------------------- #\n# Feed Forward, Layer Norm and Add & Norm\n\nprint(\"INFO: line 282 -> FeedForward\")\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout_rate: float):\n        super().__init__()\n\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_1 = nn.Linear(d_model, d_ff)\n        # self.activation_1 = nn.ReLU()\n        self.layer_2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, input):\n        # return self.layer_2(self.dropout(self.activation_1(self.layer_1(input))))\n        return self.layer_2(self.dropout(torch.relu(self.layer_1(input))))  # NOTE: relu is used instead of activation_1\n\n    \nprint(\"INFO: line 298 -> LayerNorm\")\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, eps: float = 1e-5):\n        super().__init__()\n        # Epselon helps prevent potential division by 0\n        self.eps = eps\n\n        # Extra learning parameters gamma and beta are introduced to scale and shift the embedding value as the network needed.\n        self.gamma = nn.Parameter(torch.ones(512)) # 512 is advisable to be the same as d_model\n        self.beta = nn.Parameter(torch.zeros(512))\n\n    def forward(self, input):\n        mean = input.mean(dim=-1, keepdim=True)\n        std = input.std(dim=-1, keepdim=True)\n\n        return self.gamma * (input - mean)/(std + self.eps) + self.beta\n\n\nprint(\"INFO: line 317 -> AddAndNorm\")\n\n\nclass AddAndNorm(nn.Module):\n    def __init__(self, dropout_rate: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_norm = LayerNorm()\n\n    def forward(self, input, sub_layer):\n        return input + self.dropout(sub_layer(self.layer_norm(input)))\n    \n    \n# --------------------------------------------------- #\n# Encode block and Encoder\n\nprint(\"INFO: line 333 -> EncoderBlock\")\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float) -> None:\n        super().__init__()\n        self.multihead_attention = multihead_attention\n        self.feed_forward = feed_forward\n        # self.add_and_norm_list = nn.ModuleList(\n        #     [AddAndNorm(dropout_rate) for _ in range(2)]\n        # )  # 2 Add & Norm layers for every Encoder Block\n        self.addnorm_1 = AddAndNorm(dropout_rate)\n        self.addnorm_2 = AddAndNorm(dropout_rate)\n\n    def forward(self, encoder_input, encoder_mask):\n        # First AddAndNorm unit taking encoder input from skip connection and adding it with the output of MultiHead attention block\n        encoder_input = self.addnorm_1(encoder_input, lambda encoder_input: self.multihead_attention(encoder_input, encoder_input, encoder_input, encoder_mask))\n        # Second AddAndNorm unit taking output of MultiHead attention block from skip connection and adding it with the output of Feedforward layer\n        encoder_input = self.addnorm_2(encoder_input, self.feed_forward)\n        \n        return encoder_input\n    \n    \nprint(\"INFO: line 353 -> Encoder\")\n\n\nclass Encoder(nn.Module):\n    def __init__(self, encoderblocklist: nn.ModuleList):\n        super().__init__()\n\n        self.encoderblocklist = encoderblocklist\n        self.layer_norm = LayerNorm()\n\n    def forward(self, encoder_input, encoder_mask):\n        # loop through the encoderblocklist - 6 blocks\n        for encoderblock in self.encoderblocklist:\n            encoder_input = encoderblock(encoder_input, encoder_mask)\n\n        # Normalize the final encoder block output and return. This encoder output will be used later on as key and value for the cross attention in decoder block.\n        encoder_output = self.layer_norm(encoder_input)\n\n        return encoder_output\n    \n\n# --------------------------------------------------- #\n# Decoder block, Decoder and Projection\n\nprint(\"INFO: line 378 -> DecoderBlock\")\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, masked_multihead_attention: MultiHeadAttention, cross_multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float) -> None:\n        super().__init__()\n        self.masked_multihead_attention = masked_multihead_attention\n        self.cross_multihead_attention = cross_multihead_attention\n        self.feed_forward = feed_forward\n        # self.add_and_norm_list = nn.ModuleList(\n        #     [AddAndNorm(dropout_rate) for _ in range(3)]\n        # )\n        self.addnorm_1 = AddAndNorm(dropout_rate)\n        self.addnorm_2 = AddAndNorm(dropout_rate)\n        self.addnorm_3 = AddAndNorm(dropout_rate)\n\n    def forward(self, decoder_input, encoder_output, encoder_mask, decoder_mask):\n        # First AddAndNorm unit taking decoder input from skip connection and adding it with the output of Masked Multi-Head attention block\n        decoder_input = self.addnorm_1(decoder_input, lambda decoder_input: self.masked_multihead_attention(decoder_input, decoder_input, decoder_input, decoder_mask))\n        # Second AddAndNorm unit taking output of Masked Multi-Head attention block from skip connection and adding it with the output of MultiHead attention block\n        decoder_input = self.addnorm_2(decoder_input, lambda decoder_input: self.cross_multihead_attention(decoder_input, encoder_output, encoder_output, encoder_mask))\n        # Third AddAndNorm unit taking output of MultiHead attention block from skip connection and adding it with the output of Feedforward layer\n        decoder_input = self.addnorm_3(decoder_input, self.feed_forward)\n\n        return decoder_input\n    \n    \nprint(\"INFO: line 407 -> Decoder\")\n\n\nclass Decoder(nn.Module):\n    def __init__(self, decoderblocklist: nn.ModuleList):\n        super().__init__()\n\n        self.decoderblocklist = decoderblocklist\n        self.layer_norm = LayerNorm()\n\n    def forward(self, decoder_input, encoder_output, encoder_mask, decoder_mask):\n        for decoderblock in self.decoderblocklist:\n            decoder_input = decoderblock(decoder_input, encoder_output, encoder_mask, decoder_mask)\n\n        decoder_output = self.layer_norm(decoder_input)\n\n        return decoder_output\n    \n    \nprint(\"INFO: line 425 -> ProjectionLayer\")\n\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, d_model, vocab_size):\n        super().__init__()\n        self.projection_layer = nn.Linear(d_model, vocab_size)\n\n    def forward(self, decoder_output):\n        # Projection layer first take in decoder output and passed into the linear layer of shape (d_model, vocab_size)\n        # Change in shape: decoder_output(batch_size, seq_len, d_model) @ linear_layer(d_model, vocab_size) => output(batch_size, seq_len, vocab_size)\n        output = self.projection_layer(decoder_output)\n\n        # softmax function to output the probability distribution over the vocabulary\n        # return torch.log_softmax(output, dim=-1)\n        return output\n\n    \n    \n# --------------------------------------------------- #\n# Transformer\n\nprint(\"INFO: line 445 -> Transformer\")\n\n\nclass Transformer(nn.Module):\n    def __init__(self,\n                 encoder: Encoder, \n                 decoder: Decoder, \n                 source_embed: EmbeddingLayer, \n                 target_embed: EmbeddingLayer, \n                 source_pos: PositionalEncoding, \n                 target_pos: PositionalEncoding, \n                 projection_layer: ProjectionLayer\n    ) -> None:\n        super().__init__()\n\n        self.source_embed = source_embed\n        self.source_pos = source_pos\n        self.encoder = encoder\n\n        self.target_embed = target_embed\n        self.target_pos = target_pos\n        self.decoder = decoder\n\n        self.projection_layer = projection_layer\n\n    # Encode function takes in encoder input, does necessary processing inside all encoder blocks and gives encoder output.\n    def encode(self, encoder_input, encoder_mask):\n        encoder_input = self.source_embed(encoder_input)\n        encoder_input = self.source_pos(encoder_input)\n        encoder_output = self.encoder(encoder_input, encoder_mask)\n        return encoder_output\n\n    # Decode function takes in decoder input, does necessary processing inside all decoder blocks and gives decoder output.\n    def decode(self, encoder_output, encoder_mask, decoder_input, decoder_mask):\n        decoder_input = self.target_embed(decoder_input)\n        decoder_input = self.target_pos(decoder_input)\n        decoder_output = self.decoder(decoder_input, encoder_output, encoder_mask, decoder_mask)\n        return decoder_output\n\n    # Projec function takes in decoder output into its projection layer and maps the output to the vocabulary for prediction.\n    def project(self, decoder_output):\n        return self.projection_layer(decoder_output)\n    \n    \n# INFO: BUILD MODEL BLOCK INFO:\n\nprint(\"INFO: line 497 -> Model build started.\")\n\n\ndef build_model(\n    source_vocab_size: int, \n    target_vocab_size: int, \n    source_seq_len: int, \n    target_seq_len: int, \n    d_model: int=512, \n    num_blocks: int=6, \n    num_heads: int=8, \n    dropout_rate: float=0.1, \n    d_ff: int=2048\n) -> Transformer:\n    # Design and assign all the values that are needed by the transformer architecture\n    source_embed = EmbeddingLayer(d_model, source_vocab_size)\n    target_embed = EmbeddingLayer(d_model, target_vocab_size)\n    \n    # Create the positional encoding layers\n    source_pos = PositionalEncoding(d_model, source_seq_len, dropout_rate)\n    target_pos = PositionalEncoding(d_model, target_seq_len, dropout_rate)\n    \n    # Create the encoder-block-list\n    encoderblocklist = []\n    for _ in range(num_blocks):\n        multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n        feed_forward = FeedForward(d_model, d_ff, dropout_rate)\n        encoder_block = EncoderBlock(multihead_attention, feed_forward, dropout_rate)\n        encoderblocklist.append(encoder_block)\n    # Create the encoder\n    encoder = Encoder(nn.ModuleList(encoderblocklist))\n\n    # Create the decoder-block-list\n    decoderblocklist = []\n    for _ in range(num_blocks):\n        masked_multihead_attention = MultiHeadAttention(d_model,num_heads, dropout_rate)\n        cross_multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n        feed_forward = FeedForward(d_model, d_ff, dropout_rate)\n        decoder_block = DecoderBlock(masked_multihead_attention, cross_multihead_attention, feed_forward, dropout_rate)\n        decoderblocklist.append(decoder_block)\n    # Create the decoder\n    decoder = Decoder(nn.ModuleList(decoderblocklist))\n\n    # Create the projection layer\n    projection_layer = ProjectionLayer(d_model, target_vocab_size)\n\n    # Now that we've initialized all the required blocks of transformer, we can now inititiate a model\n    model = Transformer(\n        encoder, \n        decoder, \n        source_embed, \n        target_embed, \n        source_pos, \n        target_pos, \n        projection_layer\n    )\n\n    for param in model.parameters():\n        if param.dim() > 1:\n            nn.init.xavier_uniform_(param)\n\n    return model\n\n\n\n# Finally, call build model and assign it to model variable.\n# This model is now fully ready to train and validate our dataset.\n# After training and validation, we can perform new translation task using this very model\n\n# Let's build the the final model.\nmodel = build_model(\n    tokenizer_en.get_vocab_size(), \n    tokenizer_hi.get_vocab_size(),\n    max_seq_len, max_seq_len, \n    d_model=512\n).to(device)\n\n# Let's look at the architecture that we've just build ourself\nprint(model)\nwandb.watch(model)\n\n# INFO: END BUILD MODEL BLOCK INFO:\nprint(\"INFO: line 562 -> Model build completed.\")\n\n\n# INFO: TRAIN MODEL BLOCK INFO:\n\n\ndef run_validation(model, validation_ds, tokenizer_en, tokenizer_hi, max_seq_len, device, print_msg, global_step):\n    model.eval()\n    count = 0\n\n    with torch.no_grad():\n        for batch in validation_ds:\n            count += 1\n            encoder_input = batch[\"encoder_input\"].to(device)\n            encoder_mask = batch[\"encoder_mask\"].to(device)\n\n            cls_id = tokenizer_hi.token_to_id('[CLS]')\n            sep_id = tokenizer_hi.token_to_id('[SEP]')\n            \n            with autocast():\n                # Computing the output of the encoder for the source sequence\n                encoder_output = model.module.encode(encoder_input, encoder_mask)\n            # for prediction task, the first token that goes in decoder input is the [CLS] token\n            decoder_input = torch.empty(1, 1).fill_(cls_id).type_as(encoder_input).to(device)\n            # since we need to keep adding the output back to the input until the [SEP] - end token is received.\n            while True:\n                # check if the max length is received\n                if decoder_input.size(1) == max_seq_len:\n                    break\n\n                # recreate mask each time the new output is added the decoder input for next token prediction\n                decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n\n                with autocast():\n                    # apply projection only to the next token\n                    out = model.module.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n\n                    # apply projection only to the next token\n                    prob = model.module.project(out[:, -1])\n\n                    # select the token with highest probablity which is a greedy search implementation\n                    _, next_word = torch.max(prob, dim=1)\n                    #print(\"next word shape: \", next_word.shape)\n                    next_word = next_word.unsqueeze(0)\n#                 next_word = next_word.unsqueeze(0).transpose(0, 1).unsqueeze(0)\n                #print(\"next word shape: \", next_word.shape)\n                \n#                 decoder_input = torch.cat(\n#                     [decoder_input, torch.empty(1, 1).type_as(encoder_input).fill_(next_word.item()).to(device)], dim=1\n#                 )\n                #print(\"decoder input shape: \", decoder_input.shape)\n#                 decoder_input = torch.cat(\n#                     [decoder_input, next_word], dim=1\n#                 )\n                    # Ensure decoder_input and next_word have compatible dimensions\n                decoder_input = torch.cat(\n                        [decoder_input, next_word], dim=1\n                )\n                # check if the new token is the end of token\n                if (next_word == sep_id).any():\n                    break\n            # final output is the concatinated decoder input till the end token is reached\n            model_out = decoder_input.squeeze(0)\n\n            source_text = batch[\"source_text\"][0]\n            target_text = batch[\"target_text\"][0]\n            model_out_text = tokenizer_hi.decode(model_out.detach().cpu().numpy())\n\n            # Print the source, target and model output\n            print_msg('-'*55)\n            # print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n            # print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n            # print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n            print_msg(f'Source Text: {source_text}')\n            print_msg(f'Target Text: {target_text}')\n            print_msg(f'Predicted by langGPT: {model_out_text}')\n\n            if count == 2:\n                break\n\n\n# Constants\nEPOCHS = 20\nBATCH_SIZE = 1\nACCUMULATION_STEPS = 4\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nprint(\"INFO: Dataloader started.\")\n# Creating DataLoader wrapper for both training and validation dataset. This dataloader will be used later stage during training and validation of our LLM model.\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\nprint(\"INFO: Dataloader complete.\")\n\n# Adam is one of the most commonly used optimization algorithms that hold the current state and will update the parameters based on the computed gradients.\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, eps=1e-9)\nscaler = GradScaler()\n\n# Learning Rate Scheduler\nnum_training_steps = EPOCHS * len(train_dataloader)\nlr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# The CrossEntropyLoss loss function computes the difference between the projection output and target label.\nloss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_en.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n\n# Multi-GPU\nprint(\"GPU count: \", torch.cuda.device_count())\nif torch.cuda.device_count() > 1:\n    model = torch.nn.DataParallel(model)\n\nmodel = model.to(device)\n\n# Function to clear GPU cache\ndef clear_gpu_cache():\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n\n\ndef train_model(preload_epoch=None):\n    # The entire training, validation cycle will run for 20 cycles or epochs.\n    EPOCHS = 20\n    initial_epoch = 0\n    global_step = 0\n    epoch_loss = 0\n\n    # If the preload_epoch is not none, that means the training will start with the weights, optimizer that has been last saved and start with preload epoch + 1\n    if preload_epoch is not None:\n        model_filename = f\"/kaggle/working/model_{preload_epoch}.pt\"\n        state = torch.load(model_filename)\n        model.load_state_dict(state['model_state_dict'])\n        initial_epoch = state['epoch'] + 1\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n      \n        # Load the scheduler state dict if it exists in the checkpoint\n        if 'scheduler_state_dict' in state and state['scheduler_state_dict'] is not None:\n            lr_scheduler.load_state_dict(state['scheduler_state_dict'])\n\n    for epoch in range(initial_epoch, EPOCHS):\n        # torch.cuda.empty_cache()\n        model.train()\n        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n        optimizer.zero_grad()\n        \n        for batch in batch_iterator:\n            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n            target_label = batch['target_label'].to(device) # (B, seq_len)\n            \n            with autocast():\n                # Run the tensors through the encoder, decoder and the projection layer\n                encoder_output = model.module.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n                decoder_output = model.module.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n                projection_output = model.module.project(decoder_output) # (B, seq_len, vocab_size)\n\n                # Compute the loss using a simple cross entropy\n                loss = loss_fn(projection_output.view(-1, tokenizer_hi.get_vocab_size()), target_label.view(-1))\n                \n                \n            scaler.scale(loss).backward()\n            \n            if (global_step + 1) % ACCUMULATION_STEPS == 0:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad(set_to_none=True)\n                lr_scheduler.step()\n                \n                # Gradient clipping\n#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                \n            \n            global_step += 1\n            \n            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n            epoch_loss = loss.item()\n            wandb.log({\"loss\": loss.item()}, step=global_step)\n            clear_gpu_cache()\n\n#             # Backpropagate the loss\n#             loss.backward()\n\n#             # Update the weights\n#             optimizer.step()\n#             optimizer.zero_grad(set_to_none=True)\n\n            \n        \n        wandb.log({\"epoch loss\": epoch_loss}, step=epoch)\n        # VALIDATION BLOCK STARTS HERE [Runs every epoch after the training block is complete]\n        run_validation(model, val_dataloader, tokenizer_en, tokenizer_hi, max_seq_len, device, lambda msg: batch_iterator.write(msg), global_step)\n\n        # Save the model at the end of every epoch\n        model_filename = f\"/kaggle/working/model_{epoch}.pt\"\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step,\n            'scheduler_state_dict': lr_scheduler.state_dict() if lr_scheduler else None,\n        }, model_filename)\n\n# Train our model\n\n# This function runs the training and validation for 10 epochs\ntrain_model(preload_epoch=4)\nprint(\"INFO: Model training completed.\")\nprint(f\"INFO: Time: {time.time()}\")\n\nprint(\n    f\"INFO: Total time taken(including loading dataset, training tokenizer, building the model, validating the model): {time.time() - st_time}s\"\n)\n\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:17:27.272506Z","iopub.execute_input":"2024-07-12T13:17:27.272861Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"INFO: device = cuda\nINFO: line:23 -> EncodeDataset\nINFO: Encoding dataset started.\nINFO: Encoding dataset complete.\nINFO: line 152 -> EmbeddingLayer\nINFO: line 169 -> PositionalEncoding\nINFO: line 208 -> MultiHeadAttention\nINFO: line 282 -> FeedForward\nINFO: line 298 -> LayerNorm\nINFO: line 317 -> AddAndNorm\nINFO: line 333 -> EncoderBlock\nINFO: line 353 -> Encoder\nINFO: line 378 -> DecoderBlock\nINFO: line 407 -> Decoder\nINFO: line 425 -> ProjectionLayer\nINFO: line 445 -> Transformer\nINFO: line 497 -> Model build started.\nTransformer(\n  (source_embed): EmbeddingLayer(\n    (embedding): Embedding(30000, 512)\n  )\n  (source_pos): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): Encoder(\n    (encoderblocklist): ModuleList(\n      (0-5): 6 x EncoderBlock(\n        (multihead_attention): MultiHeadAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (W_q): Linear(in_features=512, out_features=512, bias=False)\n          (W_k): Linear(in_features=512, out_features=512, bias=False)\n          (W_v): Linear(in_features=512, out_features=512, bias=False)\n          (W_o): Linear(in_features=512, out_features=512, bias=False)\n        )\n        (feed_forward): FeedForward(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_1): Linear(in_features=512, out_features=2048, bias=True)\n          (layer_2): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (addnorm_1): AddAndNorm(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm()\n        )\n        (addnorm_2): AddAndNorm(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm()\n        )\n      )\n    )\n    (layer_norm): LayerNorm()\n  )\n  (target_embed): EmbeddingLayer(\n    (embedding): Embedding(30000, 512)\n  )\n  (target_pos): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): Decoder(\n    (decoderblocklist): ModuleList(\n      (0-5): 6 x DecoderBlock(\n        (masked_multihead_attention): MultiHeadAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (W_q): Linear(in_features=512, out_features=512, bias=False)\n          (W_k): Linear(in_features=512, out_features=512, bias=False)\n          (W_v): Linear(in_features=512, out_features=512, bias=False)\n          (W_o): Linear(in_features=512, out_features=512, bias=False)\n        )\n        (cross_multihead_attention): MultiHeadAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (W_q): Linear(in_features=512, out_features=512, bias=False)\n          (W_k): Linear(in_features=512, out_features=512, bias=False)\n          (W_v): Linear(in_features=512, out_features=512, bias=False)\n          (W_o): Linear(in_features=512, out_features=512, bias=False)\n        )\n        (feed_forward): FeedForward(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_1): Linear(in_features=512, out_features=2048, bias=True)\n          (layer_2): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (addnorm_1): AddAndNorm(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm()\n        )\n        (addnorm_2): AddAndNorm(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm()\n        )\n        (addnorm_3): AddAndNorm(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm()\n        )\n      )\n    )\n    (layer_norm): LayerNorm()\n  )\n  (projection_layer): ProjectionLayer(\n    (projection_layer): Linear(in_features=512, out_features=30000, bias=True)\n  )\n)\nINFO: line 562 -> Model build completed.\nUsing device: cuda\nINFO: Dataloader started.\nINFO: Dataloader complete.\nGPU count:  2\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 05:   0%|          | 0/50000 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nProcessing Epoch 05:  14%|█▍        | 7001/50000 [15:03<2:00:32,  5.95it/s, loss=4.820]","output_type":"stream"}]},{"cell_type":"code","source":"! ls /kaggle/working/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}