{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport time\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.trainers import BpeTrainer\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-07-10T17:22:25.058914Z","iopub.execute_input":"2024-07-10T17:22:25.059638Z","iopub.status.idle":"2024-07-10T17:22:32.861662Z","shell.execute_reply.started":"2024-07-10T17:22:25.059605Z","shell.execute_reply":"2024-07-10T17:22:32.860514Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"wandb.login()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T17:24:50.884665Z","iopub.execute_input":"2024-07-10T17:24:50.885109Z","iopub.status.idle":"2024-07-10T17:24:59.663301Z","shell.execute_reply.started":"2024-07-10T17:24:50.885076Z","shell.execute_reply":"2024-07-10T17:24:59.662073Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"wandb.init(\n    project=\"LangGPT\",\n    config={\n        \"architecture\": \"Transformers\",\n        \"dataset\": \"https://huggingface.co/datasets/cfilt/iitb-english-hindi\",\n        \"epochs\": 10,\n        \"Training Data\": 100,\n        \"Validation Data\": 50,\n        \"Version\": \"V1\",\n        \"tags\": \"Kaggle test run\",\n    },\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T17:43:35.289571Z","iopub.execute_input":"2024-07-10T17:43:35.290037Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:cpvie8v6) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.017 MB of 0.017 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">silver-galaxy-15</strong> at: <a href='https://wandb.ai/srikarv44/LangGPT/runs/cpvie8v6' target=\"_blank\">https://wandb.ai/srikarv44/LangGPT/runs/cpvie8v6</a><br/> View project at: <a href='https://wandb.ai/srikarv44/LangGPT' target=\"_blank\">https://wandb.ai/srikarv44/LangGPT</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240710_172600-cpvie8v6/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:cpvie8v6). Initializing new run:<br/>"},"metadata":{}}]},{"cell_type":"code","source":"# :: DATASET ::\n# Download dataset from Hugging-face: https://huggingface.co/datasets/cfilt/iitb-english-hindi\nprint(\"INFO: Dataset download started.\")\nraw_train_dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=\"train\")\nraw_val_dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=\"validation\")\nraw_test_dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=\"test\")\nprint(\"INFO: Dataset download complete.\")\n\n\n# Splitting the dataset into training and validation dataset of 3000 and 300 respectively for faster training and validation.\nraw_train_dataset, rt_to_skip = random_split(raw_train_dataset, [100, len(raw_train_dataset) - 100])\nraw_val_dataset, vt_to_skip = random_split(raw_val_dataset, [50, len(raw_val_dataset) - 50])\n\n\n# :: TOKENIZER :: \n# [ Creating Source Language Tokenizer - English ]\n# Additional Special Tokens: [UNK] - to represent Unknown words, [PAD] - to represent padding added to keep sequence length constant for the model\n# [CLS] - Token to denote start of sentence, [SEP] = Token to denote end of sentence\n\ntokenizer_en = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer_en = BpeTrainer(\n    min_frequency=2, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n)\n\n# NOTE: below function is used as an iterator on the smaller random dataset we just \ndef get_ds_iterator(raw_train_dataset, lang):\n  for data in raw_train_dataset:\n    yield data['translation'][lang]\n    \n\n# splitting tokens based on whitespaces\ntokenizer_en.pre_tokenizer = Whitespace()\nprint(\"INFO: source tokenizer initialized\")\n\nprint(\"INFO: source tokenizer training started...\")\nstart_time = time.time()\ntokenizer_en.train_from_iterator(get_ds_iterator(raw_train_dataset, \"en\"), trainer=trainer_en)\n# tokenizer_en.train(files=path_en, trainer=trainer_en)\nprint(\"INFO: source tokenizer training completed!\")\nprint(f\"INFO: time taken: {time.time() - start_time}s\")\n\n\n# Save tokenizer for future use\ntokenizer_en.save(\"models/tokenizer_en.json\")\nprint(\n    f\"INFO: source tokenizer saved into: /models/tokenizer_en/tokenizer_en.json\"\n)\n\n\n# [ Creating Target Language Tokenizer - Hindi ]\ntokenizer_hi = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer_hi = BpeTrainer(\n    min_frequency=2, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n)\n\n# splitting tokens based on whitespaces\ntokenizer_hi.pre_tokenizer = Whitespace()\nprint(\"INFO: target tokenizer initialized\")\n\nprint(\"INFO: target tokenizer training started...\")\nstart_time = time.time()\ntokenizer_hi.train_from_iterator(get_ds_iterator(raw_train_dataset, \"hi\"), trainer=trainer_hi)\n# tokenizer_hi.train(files=path_hi, trainer=trainer_hi)\nprint(\"INFO: target tokenizer training completed!\")\nprint(f\"INFO: time taken: {time.time() - start_time}s\")\n\n# Save tokenizer for future use\ntokenizer_hi.save(\"models/tokenizer_hi.json\")\nprint(\n    f\"INFO: source tokenizer saved into: /models/tokenizer_hi/tokenizer_hi.json\"\n)\n\n# Load tokenizers from file\ntokenizer_en = Tokenizer.from_file(\"models/tokenizer_en.json\")\ntokenizer_hi = Tokenizer.from_file(\"models/tokenizer_hi.json\")\n\n# Store the vocab size of source and target tokenizers\nsource_vocab_size = tokenizer_en.get_vocab_size()\ntarget_vocab_size = tokenizer_hi.get_vocab_size()\nprint(f\"INFO: source tokenizer vocab size = {source_vocab_size}\")\nprint(f\"INFO: target tokenizer vocab size = {target_vocab_size}\")\n\n\n\n# :: TRAINING ::\nst_time = time.time()\n\n# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"INFO: line:23 -> EncodeDataset\")\n# This class takes raw dataset and max_seq_len\nclass EncodeDataset(Dataset):\n    def __init__(self, raw_dataset, max_seq_len):\n        \"\"\"\n        Constructor to initialise class variables\n\n        Args:\n            raw_dataset (Dataset): raw data downloaded from hugging-face\n            max_seq_len (int): max seq length of the sentences in the dataset\n        \"\"\"\n        super().__init__()\n        self.raw_dataset = raw_dataset\n        self.max_seq_len = max_seq_len\n\n    def __len__(self):\n        return len(self.raw_dataset)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Generating below for the translation pair at raw_dataset[index]:\n        - encoder_input\n        - decoder_input\n        - target_label\n        - encoder_mask\n        - decoder_mask\n        - source_text\n        - target_text\n\n        Args:\n            index (_type_): item at index in raw_dataset\n        \"\"\"\n\n        # Fetch the raw translation\n        raw_text = self.raw_dataset[index]\n\n        # Split into source and target text\n        source_text = raw_text['translation']['en']\n        target_text = raw_text['translation']['hi']\n\n        # Encoding source text using source tokenizer(tokenizer_en) and target text using target tokenizer(tokenizer_hi)\n        source_text_encoded = tokenizer_en.encode(source_text).ids\n        target_text_encoded = tokenizer_hi.encode(target_text).ids\n\n        # Convert the CLS, SEP and PAD tokens to their corresponding index id in vocabulary using tokenizer [the id would be same with either tokenizers]\n        CLS_ID = torch.tensor([tokenizer_hi.token_to_id(\"[CLS]\")], dtype=torch.int64)\n        SEP_ID = torch.tensor([tokenizer_hi.token_to_id(\"[SEP]\")], dtype=torch.int64)\n        PAD_ID = torch.tensor([tokenizer_hi.token_to_id(\"[PAD]\")], dtype=torch.int64)\n        \n        # To train the model we have to same sequence length for input and output and hence we need to add padding\n        # Calculate the number of padding to be added for source and target\n        num_source_padding = (\n            self.max_seq_len - len(source_text_encoded) - 2\n        )  # 2 -> [CLS] and [SEP]\n        num_target_padding = (\n            self.max_seq_len - len(target_text_encoded) - 1\n        )  # 1 -> [SEP] only because target label contains [SEP] only and [CLS] is required by the model to start the inference\n\n        # Add the padding based on the number computer above\n        encoder_padding = torch.tensor([PAD_ID] * num_source_padding, dtype=torch.int64)\n        decoder_padding = torch.tensor([PAD_ID] * num_target_padding, dtype=torch.int64)\n\n        # construct the encoder input\n        # Encoder I/P: [CLS_ID] + source_text_encoded + [SEP_ID] + encoder_padding\n        encoder_input = torch.cat([CLS_ID, torch.tensor(source_text_encoded, dtype=torch.int64), SEP_ID, encoder_padding], dim=0)\n\n        # construct the decoder input\n        # Decoder I/P: [CLS_ID] + target_text_encoded + decoder_padding\n        decoder_input = torch.cat([CLS_ID, torch.tensor(target_text_encoded, dtype=torch.int64), decoder_padding ], dim=0)\n\n        # construct the target label\n        # Target Label: target_text_encoded + [SEP_ID] + decoder_padding\n        target_label = torch.cat([torch.tensor(target_text_encoded, dtype=torch.int64), SEP_ID, decoder_padding], dim=0)\n\n        # As we are adding extra padding to match the sequence input,but we should not let the model train on it\n        # hence, we'll use encoder mask to nullify the padding tokens\n        encoder_mask = (\n            (encoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int()\n        )\n\n        # We'll do that same for decoder too but we also need to get rid of the upper triangle for Masked Multi-Attention\n        decoder_mask = (decoder_input != PAD_ID).unsqueeze(0).unsqueeze(\n            0\n        ).int() & causal_mask(decoder_input.size(0))\n\n        return {\n            \"encoder_input\": encoder_input,\n            \"decoder_input\": decoder_input,\n            \"target_label\": target_label,\n            \"encoder_mask\": encoder_mask,\n            \"decoder_mask\": decoder_mask,\n            \"source_text\": source_text,\n            \"target_text\": target_text,\n        }\n\n\n# Causal mask will make sure any token that comes after the current token will be masked, meaning the value will be replaced by -ve infinity which will be converted to zero or close to zero after softmax function.\n# Hence the model will just ignore these value or willn't be able to learn anything from these values.\ndef causal_mask(size):\n    # dimension of causal mask (batch_size, seq_len, seq_len)\n    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n    return mask == 0\n\n\n# calculating max_seq_len from the dataset\nmax_seq_len_source = 0\nmax_seq_len_target = 0\n\nfor data in raw_train_dataset:\n    enc_ids = tokenizer_en.encode(data['translation']['en']).ids\n    dec_ids = tokenizer_hi.encode(data['translation']['hi']).ids\n    max_seq_len_source = max(max_seq_len_source, len(enc_ids))\n    max_seq_len_target = max(max_seq_len_target, len(dec_ids))\n\n\nprint(f\"Max sequence length of source: {max_seq_len_source}\")  # 50\nprint(f\"Max sequence length of target: {max_seq_len_target}\")  # 50\n\n\n# To simplify the calcualtion let's add some value to the greater value and have a single max_seq_len\n#max_seq_len = 100  # 50 + 20: 20 -> to accomodate the additional length of tokens such as PAD, CLS, SEP in the sequence.\nmax_seq_len = max(max_seq_len_source, max_seq_len_target) + 20\n\n\nprint(\"INFO: Encoding dataset started.\")\n# Instantiate the EncodeDataset class and create the encoded train and validation-dataset.\ntrain_dataset = EncodeDataset(raw_train_dataset, max_seq_len)\nval_dataset = EncodeDataset(raw_val_dataset, max_seq_len)\nprint(\"INFO: Encoding dataset complete.\")\n\nprint(\"INFO: Dataloader started.\")\n# Creating DataLoader wrapper for both training and validation dataset. This dataloader will be used later stage during training and validation of our LLM model.\ntrain_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\nprint(\"INFO: Dataloader complete.\")\n\n\n# --------------------------------------------------- #\n# Input Embedding and Positional Encoding\n\nprint(\"INFO: line 152 -> EmbeddingLayer\")\n\n\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int):\n        super().__init__()\n        self.d_model = d_model\n\n        # using pytorch's embedding module we will map the token_id with the vocabulary and then convert it to embedding matrix\n        self.embedding = nn.Embedding(\n            vocab_size, d_model\n        )  # initialise the Embedding layer to taken in the vocab_size and output a embeddign vector of size d_model\n\n    def forward(self, input):\n        # After the output of embedding is recieved the output is multiplied with teh sqrt(d_model) for normalizing the output\n        embedding_output = self.embedding(input) * math.sqrt(self.d_model)\n        return embedding_output\n    \n\nprint(\"INFO: line 169 -> PositionalEncoding\")\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_seq_len: int, dropout_rate: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # we're creating a zero matrix of the same size as the embedding matrix\n        pe = torch.zeros(max_seq_len, d_model)\n\n        # Calculate the position part of the PE function\n        pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n\n        # Calculate the division part of the PE function\n        # NOTE: div part expression is slightly different that papers expression as this exponential functions seems to works better.\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\n        # Fill in the `pe` with the sin and cos of the PE function\n        # NOTE: sin -> even pos\n        # NOTE: cos -> odd pos\n\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n\n        # Since we're expecting the input sequences in batches so the extra batch_size dimension is added in 0 postion.\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe) # LEARN: what is register_buffer\n\n    def forward(self, input_embdding):\n        # Add positional encoding together with the input embedding vector.\n        input_embdding = input_embdding + (\n            self.pe[:, : input_embdding.shape[1], :]\n        ).requires_grad_(False) # to prevent from calculating the gradient of the positional encoding.\n\n        # Perform dropout to prevent overfitting.\n        return self.dropout(input_embdding)\n\n    \n# --------------------------------------------------- #\n# Multi-head Attention Block\n\nprint(\"INFO: line 208 -> MultiHeadAttention\")\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout_rate: float):\n        super().__init__()\n        # Define dropout to prevent overfitting\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # Weight matrices are intoduced and all are learnable params\n        self.W_q = nn.Linear(d_model, d_model, bias=False)  # Linear -> to enable learning # NOTE: bias=False -> to prevent bias newly added\n        self.W_k = nn.Linear(d_model, d_model, bias=False)\n        self.W_v = nn.Linear(d_model, d_model, bias=False)\n        self.W_o = nn.Linear(d_model, d_model, bias=False)\n\n        self.num_heads = num_heads\n        assert d_model % num_heads == 0, \"d_model must be divisible by number of heads\"\n\n        # d_k is the new dimension of each of each splitted self-attention heads\n        self.d_k = d_model // num_heads\n\n    def forward(self, q, k, v, encoder_mask):\n\n        # # We'll be training our model with multiple batches of sequence at once in parallel, hence we'll need to include batch_size in the shape as well.\n        # query, key and value are calculated by matrix multiplication of corresponding weights with the input embeddings.\n        # Change of shape: q(batch_size, seq_len, d_model) @ W_q(d_model, d_model) => query(batch_size, seq_len, d_model) [same goes to key and value].\n        query = self.W_q(q)\n        key = self.W_k(k)\n        value = self.W_v(v)\n\n        # Splitting query, key and value into number of heads. d_model is splitted in d_k across 8 heads.\n        # Change of shape: query(batch_size, seq_len, d_model) => query(batch_size, seq_len, num_heads, d_k) -> query(batch_size,num_heads, seq_len,d_k) [same goes to key and value].\n        query = query.view(query.shape[0], query.shape[1], self.num_heads ,self.d_k).transpose(1,2)\n        key = key.view(key.shape[0], key.shape[1], self.num_heads ,self.d_k).transpose(1,2)\n        value = value.view(value.shape[0], value.shape[1], self.num_heads ,self.d_k).transpose(1,2)\n\n        # INFO: SELF-ATTENTION BLOCK STARTS INFO:\n\n        # Attention score is calculated\n        # Change of shape: query(batch_size,num_heads, seq_len,d_k) @ key(batch_size,num_heads, seq_len,d_k) => attention_score(batch_size,num_heads, seq_len,seq_len).\n        attention_score = (query @ key.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n        # If masking is available\n        if encoder_mask is not None:\n            attention_score.masked_fill_(encoder_mask==0, -1e9)\n\n        # Softmax function calculates the probability distribution among all the attention scores. It assign higher probabiliy value to higher attention score. Meaning more similar tokens get higher probability value.\n        # Change of shape: same as attention_score\n        # attention_weight = torch.softmax(attention_score, dim=-1)\n        attention_score = attention_score.softmax(dim=-1)\n\n        if self.dropout is not None:\n            attention_score = self.dropout(attention_score)\n\n        # Final step in Self attention block is, matrix multiplication of attention_weight with Value embedding vector.\n        # Change of shape: attention_score(batch_size,num_heads, seq_len,seq_len) @  value(batch_size,num_heads, seq_len,d_k) => attention_output(batch_size,num_heads, seq_len,d_k)\n        attention_output = attention_score @ value\n\n        # INFO: SELF-ATTENTION BLOCK ENDS\n\n        # Now, all heads must be combined back to a single head\n        # Change of shape:attention_output(batch_size,num_heads, seq_len,d_k) => attention_output(batch_size,seq_len,num_heads,d_k) => attention_output(batch_size,seq_len,d_model)\n        attention_output = attention_output.transpose(1,2).contiguous().view(attention_output.shape[0], -1, self.num_heads * self.d_k)\n\n        # Finally attention_output is matrix multiplied with output weight matrix to give the final Multi-Head attention output.\n        # The shape of the multihead_output is same as the embedding input\n        # Change of shape: attention_output(batch_size,seq_len,d_model) @ W_o(d_model, d_model) => multihead_output(batch_size, seq_len, d_model)\n        multihead_output = self.W_o(attention_output)\n\n        return multihead_output\n\n    \n# --------------------------------------------------- #\n# Feed Forward, Layer Norm and Add & Norm\n\nprint(\"INFO: line 282 -> FeedForward\")\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout_rate: float):\n        super().__init__()\n\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_1 = nn.Linear(d_model, d_ff)\n        # self.activation_1 = nn.ReLU()\n        self.layer_2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, input):\n        # return self.layer_2(self.dropout(self.activation_1(self.layer_1(input))))\n        return self.layer_2(self.dropout(torch.relu(self.layer_1(input))))  # NOTE: relu is used instead of activation_1\n\n    \nprint(\"INFO: line 298 -> LayerNorm\")\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, eps: float = 1e-5):\n        super().__init__()\n        # Epselon helps prevent potential division by 0\n        self.eps = eps\n\n        # Extra learning parameters gamma and beta are introduced to scale and shift the embedding value as the network needed.\n        self.gamma = nn.Parameter(torch.ones(512)) # 512 is advisable to be the same as d_model\n        self.beta = nn.Parameter(torch.zeros(512))\n\n    def forward(self, input):\n        mean = input.mean(dim=-1, keepdim=True)\n        std = input.std(dim=-1, keepdim=True)\n\n        return self.gamma * (input - mean)/(std + self.eps) + self.beta\n\n\nprint(\"INFO: line 317 -> AddAndNorm\")\n\n\nclass AddAndNorm(nn.Module):\n    def __init__(self, dropout_rate: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_norm = LayerNorm()\n\n    def forward(self, input, sub_layer):\n        return input + self.dropout(sub_layer(self.layer_norm(input)))\n    \n    \n# --------------------------------------------------- #\n# Encode block and Encoder\n\nprint(\"INFO: line 333 -> EncoderBlock\")\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float) -> None:\n        super().__init__()\n        self.multihead_attention = multihead_attention\n        self.feed_forward = feed_forward\n        # self.add_and_norm_list = nn.ModuleList(\n        #     [AddAndNorm(dropout_rate) for _ in range(2)]\n        # )  # 2 Add & Norm layers for every Encoder Block\n        self.addnorm_1 = AddAndNorm(dropout_rate)\n        self.addnorm_2 = AddAndNorm(dropout_rate)\n\n    def forward(self, encoder_input, encoder_mask):\n        # First AddAndNorm unit taking encoder input from skip connection and adding it with the output of MultiHead attention block\n        encoder_input = self.addnorm_1(encoder_input, lambda encoder_input: self.multihead_attention(encoder_input, encoder_input, encoder_input, encoder_mask))\n        # Second AddAndNorm unit taking output of MultiHead attention block from skip connection and adding it with the output of Feedforward layer\n        encoder_input = self.addnorm_2(encoder_input, self.feed_forward)\n        \n        return encoder_input\n    \n    \nprint(\"INFO: line 353 -> Encoder\")\n\n\nclass Encoder(nn.Module):\n    def __init__(self, encoderblocklist: nn.ModuleList):\n        super().__init__()\n\n        self.encoderblocklist = encoderblocklist\n        self.layer_norm = LayerNorm()\n\n    def forward(self, encoder_input, encoder_mask):\n        # loop through the encoderblocklist - 6 blocks\n        for encoderblock in self.encoderblocklist:\n            encoder_input = encoderblock(encoder_input, encoder_mask)\n\n        # Normalize the final encoder block output and return. This encoder output will be used later on as key and value for the cross attention in decoder block.\n        encoder_output = self.layer_norm(encoder_input)\n\n        return encoder_output\n    \n\n# --------------------------------------------------- #\n# Decoder block, Decoder and Projection\n\nprint(\"INFO: line 378 -> DecoderBlock\")\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, masked_multihead_attention: MultiHeadAttention, cross_multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float) -> None:\n        super().__init__()\n        self.masked_multihead_attention = masked_multihead_attention\n        self.cross_multihead_attention = cross_multihead_attention\n        self.feed_forward = feed_forward\n        # self.add_and_norm_list = nn.ModuleList(\n        #     [AddAndNorm(dropout_rate) for _ in range(3)]\n        # )\n        self.addnorm_1 = AddAndNorm(dropout_rate)\n        self.addnorm_2 = AddAndNorm(dropout_rate)\n        self.addnorm_3 = AddAndNorm(dropout_rate)\n\n    def forward(self, decoder_input, encoder_output, encoder_mask, decoder_mask):\n        # First AddAndNorm unit taking decoder input from skip connection and adding it with the output of Masked Multi-Head attention block\n        decoder_input = self.addnorm_1(decoder_input, lambda decoder_input: self.masked_multihead_attention(decoder_input, decoder_input, decoder_input, decoder_mask))\n        # Second AddAndNorm unit taking output of Masked Multi-Head attention block from skip connection and adding it with the output of MultiHead attention block\n        decoder_input = self.addnorm_2(decoder_input, lambda decoder_input: self.cross_multihead_attention(decoder_input, encoder_output, encoder_output, encoder_mask))\n        # Third AddAndNorm unit taking output of MultiHead attention block from skip connection and adding it with the output of Feedforward layer\n        decoder_input = self.addnorm_3(decoder_input, self.feed_forward)\n\n        return decoder_input\n    \n    \nprint(\"INFO: line 407 -> Decoder\")\n\n\nclass Decoder(nn.Module):\n    def __init__(self, decoderblocklist: nn.ModuleList):\n        super().__init__()\n\n        self.decoderblocklist = decoderblocklist\n        self.layer_norm = LayerNorm()\n\n    def forward(self, decoder_input, encoder_output, encoder_mask, decoder_mask):\n        for decoderblock in self.decoderblocklist:\n            decoder_input = decoderblock(decoder_input, encoder_output, encoder_mask, decoder_mask)\n\n        decoder_output = self.layer_norm(decoder_input)\n\n        return decoder_output\n    \n    \nprint(\"INFO: line 425 -> ProjectionLayer\")\n\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, d_model, vocab_size):\n        super().__init__()\n        self.projection_layer = nn.Linear(d_model, vocab_size)\n\n    def forward(self, decoder_output):\n        # Projection layer first take in decoder output and passed into the linear layer of shape (d_model, vocab_size)\n        # Change in shape: decoder_output(batch_size, seq_len, d_model) @ linear_layer(d_model, vocab_size) => output(batch_size, seq_len, vocab_size)\n        output = self.projection_layer(decoder_output)\n\n        # softmax function to output the probability distribution over the vocabulary\n        # return torch.log_softmax(output, dim=-1)\n        return output\n\n    \n    \n# --------------------------------------------------- #\n# Transformer\n\nprint(\"INFO: line 445 -> Transformer\")\n\n\nclass Transformer(nn.Module):\n    def __init__(self,\n                 encoder: Encoder, \n                 decoder: Decoder, \n                 source_embed: EmbeddingLayer, \n                 target_embed: EmbeddingLayer, \n                 source_pos: PositionalEncoding, \n                 target_pos: PositionalEncoding, \n                 projection_layer: ProjectionLayer\n    ) -> None:\n        super().__init__()\n\n        self.source_embed = source_embed\n        self.source_pos = source_pos\n        self.encoder = encoder\n\n        self.target_embed = target_embed\n        self.target_pos = target_pos\n        self.decoder = decoder\n\n        self.projection_layer = projection_layer\n\n    # Encode function takes in encoder input, does necessary processing inside all encoder blocks and gives encoder output.\n    def encode(self, encoder_input, encoder_mask):\n        encoder_input = self.source_embed(encoder_input)\n        encoder_input = self.source_pos(encoder_input)\n        encoder_output = self.encoder(encoder_input, encoder_mask)\n        return encoder_output\n\n    # Decode function takes in decoder input, does necessary processing inside all decoder blocks and gives decoder output.\n    def decode(self, encoder_output, encoder_mask, decoder_input, decoder_mask):\n        decoder_input = self.target_embed(decoder_input)\n        decoder_input = self.target_pos(decoder_input)\n        decoder_output = self.decoder(decoder_input, encoder_output, encoder_mask, decoder_mask)\n        return decoder_output\n\n    # Projec function takes in decoder output into its projection layer and maps the output to the vocabulary for prediction.\n    def project(self, decoder_output):\n        return self.projection_layer(decoder_output)\n    \n    \n# INFO: BUILD MODEL BLOCK INFO:\n\nprint(\"INFO: line 497 -> Model build started.\")\n\n\ndef build_model(\n    source_vocab_size: int, \n    target_vocab_size: int, \n    source_seq_len: int, \n    target_seq_len: int, \n    d_model: int=512, \n    num_blocks: int=6, \n    num_heads: int=8, \n    dropout_rate: float=0.1, \n    d_ff: int=2048\n) -> Transformer:\n    # Design and assign all the values that are needed by the transformer architecture\n    source_embed = EmbeddingLayer(d_model, source_vocab_size)\n    target_embed = EmbeddingLayer(d_model, target_vocab_size)\n    \n    # Create the positional encoding layers\n    source_pos = PositionalEncoding(d_model, source_seq_len, dropout_rate)\n    target_pos = PositionalEncoding(d_model, target_seq_len, dropout_rate)\n    \n    # Create the encoder-block-list\n    encoderblocklist = []\n    for _ in range(num_blocks):\n        multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n        feed_forward = FeedForward(d_model, d_ff, dropout_rate)\n        encoder_block = EncoderBlock(multihead_attention, feed_forward, dropout_rate)\n        encoderblocklist.append(encoder_block)\n    # Create the encoder\n    encoder = Encoder(nn.ModuleList(encoderblocklist))\n\n    # Create the decoder-block-list\n    decoderblocklist = []\n    for _ in range(num_blocks):\n        masked_multihead_attention = MultiHeadAttention(d_model,num_heads, dropout_rate)\n        cross_multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n        feed_forward = FeedForward(d_model, d_ff, dropout_rate)\n        decoder_block = DecoderBlock(masked_multihead_attention, cross_multihead_attention, feed_forward, dropout_rate)\n        decoderblocklist.append(decoder_block)\n    # Create the decoder\n    decoder = Decoder(nn.ModuleList(decoderblocklist))\n\n    # Create the projection layer\n    projection_layer = ProjectionLayer(d_model, target_vocab_size)\n\n    # Now that we've initialized all the required blocks of transformer, we can now inititiate a model\n    model = Transformer(\n        encoder, \n        decoder, \n        source_embed, \n        target_embed, \n        source_pos, \n        target_pos, \n        projection_layer\n    )\n\n    for param in model.parameters():\n        if param.dim() > 1:\n            nn.init.xavier_uniform_(param)\n\n    return model\n\n\n\n# Finally, call build model and assign it to model variable.\n# This model is now fully ready to train and validate our dataset.\n# After training and validation, we can perform new translation task using this very model\n\n# Let's build the the final model.\nmodel = build_model(\n    tokenizer_en.get_vocab_size(), \n    tokenizer_hi.get_vocab_size(),\n    max_seq_len, max_seq_len, \n    d_model=512\n).to(device)\n\n# Let's look at the architecture that we've just build ourself\nprint(model)\nwandb.watch(model)\n\n# INFO: END BUILD MODEL BLOCK INFO:\nprint(\"INFO: line 562 -> Model build completed.\")\n\n\n# INFO: TRAIN MODEL BLOCK INFO:\n\n\ndef run_validation(model, validation_ds, tokenizer_en, tokenizer_hi, max_seq_len, device, print_msg, global_step):\n    model.eval()\n    count = 0\n\n    with torch.no_grad():\n        for batch in validation_ds:\n            count += 1\n            encoder_input = batch[\"encoder_input\"].to(device)\n            encoder_mask = batch[\"encoder_mask\"].to(device)\n\n            cls_id = tokenizer_hi.token_to_id('[CLS]')\n            sep_id = tokenizer_hi.token_to_id('[SEP]')\n\n            # Computing the output of the encoder for the source sequence\n            encoder_output = model.encode(encoder_input, encoder_mask)\n            # for prediction task, the first token that goes in decoder input is the [CLS] token\n            decoder_input = torch.empty(1, 1).fill_(cls_id).type_as(encoder_input).to(device)\n            # since we need to keep adding the output back to the input until the [SEP] - end token is received.\n            while True:\n                # check if the max length is received\n                if decoder_input.size(1) == max_seq_len:\n                    break\n\n                # recreate mask each time the new output is added the decoder input for next token prediction\n                decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n\n                # apply projection only to the next token\n                out = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n\n                # apply projection only to the next token\n                prob = model.project(out[:, -1])\n\n                # select the token with highest probablity which is a greedy search implementation\n                _, next_word = torch.max(prob, dim=1)\n                decoder_input = torch.cat(\n                    [decoder_input, torch.empty(1, 1).type_as(encoder_input).fill_(next_word.item()).to(device)], dim=1\n                )\n                # check if the new token is the end of token\n                if next_word == sep_id:\n                    break\n            # final output is the concatinated decoder input till the end token is reached\n            model_out = decoder_input.squeeze(0)\n\n            source_text = batch[\"source_text\"][0]\n            target_text = batch[\"target_text\"][0]\n            model_out_text = tokenizer_hi.decode(model_out.detach().cpu().numpy())\n\n            # Print the source, target and model output\n            print_msg('-'*55)\n            # print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n            # print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n            # print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n            print_msg(f'Source Text: {source_text}')\n            print_msg(f'Target Text: {target_text}')\n            print_msg(f'Predicted by langGPT: {model_out_text}')\n\n            if count == 2:\n                break\n    \n\ndef train_model(preload_epoch=None):\n    # The entire training, validation cycle will run for 20 cycles or epochs.\n    EPOCHS = 10\n    initial_epoch = 0\n    global_step = 0\n\n    # Adam is one of the most commonly used optimization algorithms that hold the current state and will update the parameters based on the computed gradients.\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, eps=1e-9)\n\n    # If the preload_epoch is not none, that means the training will start with the weights, optimizer that has been last saved and start with preload epoch + 1\n    if preload_epoch is not None:\n      model_filename = f\"models/LangGPT/model_{epoch}.pt\"\n      state = torch.load(model_filename)\n      model.load_state_dict(state['model_state_dict'])\n      initial_epoch = state['epoch'] + 1\n      optimizer.load_state_dict(state['optimizer_state_dict'])\n      global_step = state['global_step']\n\n    # The CrossEntropyLoss loss function computes the difference between the projection output and target label.\n    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_en.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n\n    for epoch in range(initial_epoch, EPOCHS):\n        # torch.cuda.empty_cache()\n        model.train()\n        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n        for batch in batch_iterator:\n            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n            target_label = batch['target_label'].to(device) # (B, seq_len)\n\n            # Run the tensors through the encoder, decoder and the projection layer\n            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n            projection_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n\n            # Compute the loss using a simple cross entropy\n            loss = loss_fn(projection_output.view(-1, tokenizer_hi.get_vocab_size()), target_label.view(-1))\n            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n            wandb.log({\"loss\": loss.item()}, step=global_step)\n\n            # Backpropagate the loss\n            loss.backward()\n\n            # Update the weights\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n\n            global_step += 1\n\n        # VALIDATION BLOCK STARTS HERE [Runs every epoch after the training block is complete]\n        run_validation(model, val_dataloader, tokenizer_en, tokenizer_hi, max_seq_len, device, lambda msg: batch_iterator.write(msg), global_step)\n\n        # Save the model at the end of every epoch\n        model_filename = f\"models/LangGPT/model_{epoch}.pt\"\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step\n        }, model_filename)\n\n# Train our model\n\n# This function runs the training and validation for 10 epochs\ntrain_model(preload_epoch=None)\nprint(\"INFO: Model training completed.\")\nprint(f\"INFO: Time: {time.time()}\")\n\nprint(\n    f\"INFO: Total time taken(including loading dataset, training tokenizer, building the model, validating the model): {time.time() - st_time}s\"\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T17:28:20.916142Z","iopub.execute_input":"2024-07-10T17:28:20.916549Z","iopub.status.idle":"2024-07-10T17:28:30.122600Z","shell.execute_reply.started":"2024-07-10T17:28:20.916520Z","shell.execute_reply":"2024-07-10T17:28:30.120670Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"INFO: Dataset download started.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/3.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fc2dacddd3b4f448d34e0c6f41e207a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f6e635ebbbc4bbe8aedcbf4274a6512"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/190M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96f7412f969c4a519f15b5dd1df1263d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/85.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"037f74ccd71640efbbab1e3b6442f6fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d107a8b70f04a83a9b42122b49265ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1659083 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90fe850f04f246a3a8f5b354ea57b9d4"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Download dataset from Hugging-face: https://huggingface.co/datasets/cfilt/iitb-english-hindi\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINFO: Dataset download started.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m raw_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcfilt/iitb-english-hindi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m raw_val_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcfilt/iitb-english-hindi\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m raw_test_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcfilt/iitb-english-hindi\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:2614\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2613\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2614\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2623\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2624\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2625\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/builder.py:1027\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1026\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1027\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/builder.py:1122\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1122\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1126\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1127\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1128\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1129\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/builder.py:1882\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1880\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1881\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1882\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1883\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1884\u001b[0m     ):\n\u001b[1;32m   1885\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1886\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/builder.py:1995\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1994\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1995\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, table \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m   1996\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m writer\u001b[38;5;241m.\u001b[39m_num_bytes \u001b[38;5;241m>\u001b[39m max_shard_size:\n\u001b[1;32m   1997\u001b[0m             num_examples, num_bytes \u001b[38;5;241m=\u001b[39m writer\u001b[38;5;241m.\u001b[39mfinalize()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py:93\u001b[0m, in \u001b[0;36mParquet._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, record_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m     91\u001b[0m         parquet_file\u001b[38;5;241m.\u001b[39miter_batches(batch_size\u001b[38;5;241m=\u001b[39mbatch_size, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     92\u001b[0m     ):\n\u001b[0;32m---> 93\u001b[0m         pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrecord_batch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;66;03m# Uncomment for debugging (will print the Arrow table size and elements)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;66;03m# logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\u001b[39;00m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;66;03m# logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\u001b[39;00m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cast_table(pa_table)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}