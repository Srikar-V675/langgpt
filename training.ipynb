{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport math\nfrom pathlib import Path\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler, autocast\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.trainers import BpeTrainer\nimport wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.login()","metadata":{"execution":{"iopub.status.busy":"2024-07-11T02:27:33.372899Z","iopub.execute_input":"2024-07-11T02:27:33.374140Z","iopub.status.idle":"2024-07-11T02:27:37.580743Z","shell.execute_reply.started":"2024-07-11T02:27:33.374104Z","shell.execute_reply":"2024-07-11T02:27:37.579818Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"wandb.init(\n    project=\"LangGPT\",\n    config={\n        \"architecture\": \"Transformers\",\n        \"dataset\": \"https://huggingface.co/datasets/cfilt/iitb-english-hindi\",\n        \"epochs\": 10,\n        \"Training Data\": 100,\n        \"Validation Data\": 50,\n        \"Version\": \"V1\",\n        \"tags\": \"Kaggle test run\",\n    },\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:11:44.193605Z","iopub.execute_input":"2024-07-11T03:11:44.194560Z","iopub.status.idle":"2024-07-11T03:12:08.707252Z","shell.execute_reply.started":"2024-07-11T03:11:44.194501Z","shell.execute_reply":"2024-07-11T03:12:08.706324Z"},"scrolled":true,"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:x1jfgevm) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>██▆▆▄▄▅▄▅▄▄█▄▂▃▅▃▃▄█▃▃▄▄▅▄▂▃▃▃▃▅▄▃▃▂▅▄▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>6.60059</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">usual-deluge-21</strong> at: <a href='https://wandb.ai/srikarv44/LangGPT/runs/x1jfgevm' target=\"_blank\">https://wandb.ai/srikarv44/LangGPT/runs/x1jfgevm</a><br/> View project at: <a href='https://wandb.ai/srikarv44/LangGPT' target=\"_blank\">https://wandb.ai/srikarv44/LangGPT</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240711_022741-x1jfgevm/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 1 is less than current step: 1232. Dropping entry: {'loss': 10.314845085144043, '_timestamp': 1720667227.9113212})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 2 is less than current step: 1232. Dropping entry: {'loss': 10.345490455627441, '_timestamp': 1720667228.9064326})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 3 is less than current step: 1232. Dropping entry: {'loss': 10.328569412231445, '_timestamp': 1720667229.859733})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 4 is less than current step: 1232. Dropping entry: {'loss': 10.26400089263916, '_timestamp': 1720667230.9133794})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 5 is less than current step: 1232. Dropping entry: {'loss': 10.222970962524414, '_timestamp': 1720667231.912457})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 6 is less than current step: 1232. Dropping entry: {'loss': 10.069555282592773, '_timestamp': 1720667232.8405235})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 7 is less than current step: 1232. Dropping entry: {'loss': 10.197578430175781, '_timestamp': 1720667233.788022})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 8 is less than current step: 1232. Dropping entry: {'loss': 10.174093246459961, '_timestamp': 1720667234.798643})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 9 is less than current step: 1232. Dropping entry: {'loss': 10.0379056930542, '_timestamp': 1720667235.8042705})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 10 is less than current step: 1232. Dropping entry: {'loss': 10.175093650817871, '_timestamp': 1720667236.7465463})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 11 is less than current step: 1232. Dropping entry: {'loss': 9.742006301879883, '_timestamp': 1720667237.698127})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 12 is less than current step: 1232. Dropping entry: {'loss': 10.207335472106934, '_timestamp': 1720667238.7063334})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 13 is less than current step: 1232. Dropping entry: {'loss': 10.034624099731445, '_timestamp': 1720667239.7160668})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 14 is less than current step: 1232. Dropping entry: {'loss': 9.239876747131348, '_timestamp': 1720667240.6492634})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 15 is less than current step: 1232. Dropping entry: {'loss': 9.801492691040039, '_timestamp': 1720667241.6077945})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 16 is less than current step: 1232. Dropping entry: {'loss': 10.046058654785156, '_timestamp': 1720667242.5894394})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 17 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667243.5968919})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 18 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667244.5398076})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 19 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667245.5125709})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 20 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667246.4975739})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 21 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667247.5040493})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 22 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667248.4438126})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 23 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667249.3873625})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 24 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667250.3757527})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 25 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667251.3768466})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 26 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667252.3147638})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 27 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667253.2732582})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 28 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667254.2691317})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 29 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667255.2814996})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 30 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667256.211753})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 31 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667257.1812754})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 32 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667258.1642127})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 33 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667259.1686404})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 34 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667260.1133573})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 35 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667261.0709865})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 36 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667262.0708494})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 37 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667263.0759017})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 38 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667264.0053878})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 39 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667264.9519815})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 40 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667265.9304566})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 41 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667266.9592323})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 42 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667267.8948817})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 43 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667268.849243})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 44 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667269.833697})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 45 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667270.8489683})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 46 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667271.7875607})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 47 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667272.7354152})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 48 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667273.7367358})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 49 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667274.745493})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 50 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667275.6808546})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 51 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667276.6436152})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 52 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667277.6773715})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 53 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667278.6849446})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 54 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667279.643325})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 55 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667280.5932405})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 56 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667281.6015475})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 57 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667282.6248744})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 58 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667283.5684118})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 59 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667284.5219095})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 60 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667285.518162})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 61 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667286.5289724})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 62 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667287.4766846})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 63 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667288.4392548})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 64 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667289.4220347})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 65 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667290.4318175})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 66 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667291.3714862})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 67 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667292.3429098})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 68 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667293.327071})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 69 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667294.3361433})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 70 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667295.2802927})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 71 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667296.24302})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 72 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667297.2550206})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 73 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667298.2625225})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 74 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667299.2104876})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 75 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667300.1915853})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 76 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667301.1796367})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 77 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667302.1918821})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 78 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667303.1282556})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 79 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667304.0938988})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 80 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667305.0872216})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 81 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667306.0948746})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 82 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667307.043248})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 83 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667308.0011103})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 84 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667308.982354})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 85 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667309.9875247})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 86 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667310.9202182})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 87 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667311.8686776})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 88 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667312.8481696})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 89 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667313.8548138})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 90 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667314.7905235})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 91 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667315.749172})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 92 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667316.7281108})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 93 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667317.7474759})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 94 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667318.681956})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 95 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667319.642606})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 96 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667320.6316526})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 97 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667321.637228})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 98 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667322.5704832})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 99 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667323.5188942})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 100 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667324.5025995})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 101 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667325.5073707})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 102 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667326.455492})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 103 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667327.4027445})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 104 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667328.394294})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 105 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667329.4011233})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 106 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667330.3533618})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 107 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667331.3042028})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 108 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667332.2875652})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 109 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667333.2937212})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 110 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667334.2270472})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 111 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667335.174612})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 112 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667336.1572757})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 113 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667337.1685007})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 114 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667338.1014335})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 115 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667339.0582309})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 116 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667340.0378218})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 117 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667341.04273})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 118 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667341.995911})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 119 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667342.9426413})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 120 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667343.9441388})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 121 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667344.9497702})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 122 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667345.907661})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 123 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667346.8556201})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 124 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667347.856639})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 125 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667348.8757236})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 126 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667349.8069844})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 127 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667350.759956})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 128 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667351.757123})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 129 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667352.7626793})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 130 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667353.6966271})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 128 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667351.757123})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 129 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667352.7626793})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 130 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667353.6966271})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 131 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667354.6466587})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 132 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667355.6272259})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 133 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667356.6337767})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 134 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667357.5882661})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 135 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667358.5352724})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 136 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667359.5374658})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 137 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667360.5448987})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 138 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667361.4812346})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 139 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667362.431927})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 140 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667363.439972})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 141 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667364.449108})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 142 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667365.406503})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 143 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667366.3568377})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 144 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667367.3408809})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 145 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667368.3498087})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 146 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667369.2950723})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 147 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667370.2588243})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 148 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667371.2664652})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 156 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667379.108806})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 157 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667380.114703})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 158 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667381.0548756})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 159 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667382.0050297})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 160 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667383.0070896})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 161 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667384.018987})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 162 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667384.956071})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 163 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667385.9071276})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 164 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667386.887047})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 165 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667387.8953469})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 166 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667388.853761})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 167 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667389.8039877})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 168 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667390.7928002})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 169 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667391.8181658})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 170 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667392.7538214})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 171 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667393.7032602})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 172 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667394.684746})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 173 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667395.6983404})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 174 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667396.6387203})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 175 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667397.590603})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 176 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667398.5832098})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 177 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667399.5905523})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 178 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667400.5257258})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 179 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667401.4850955})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 180 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667402.468313})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 181 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667403.4743714})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 182 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667404.408549})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 183 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667405.3552299})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 184 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667406.336249})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 185 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667407.365745})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 186 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667408.3093214})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 187 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667409.2701547})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 188 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667410.2568583})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 189 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667411.2642896})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 190 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667412.209167})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 191 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667413.1590014})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 192 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667414.1399128})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 193 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667415.145954})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 194 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667416.0846322})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 195 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667417.037612})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 196 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667418.026199})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 197 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667419.0341172})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 198 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667419.9684565})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 199 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667420.922597})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 200 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667421.903017})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 201 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667422.9207065})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 202 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667423.8547423})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 203 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667424.8114252})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 204 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667425.7919})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 205 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667426.8012328})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 206 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667427.7358813})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 207 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667428.7032158})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 208 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667429.689374})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 209 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667430.7052708})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 210 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667431.657086})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 211 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667432.6086895})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 212 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667433.591727})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 213 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667434.599378})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 214 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667435.5337481})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 215 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667436.4821794})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 216 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667437.4678867})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 217 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667438.4787686})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 218 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667439.4136205})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 219 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667440.364853})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 220 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667441.3525517})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 221 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667442.3660007})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 222 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667443.322941})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 223 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667444.2835813})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 224 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667445.273453})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 225 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667446.2800076})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 226 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667447.2258604})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 227 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667448.1769922})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 228 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667449.1746714})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 229 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667450.1821885})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 230 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667451.1183136})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 231 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667452.066058})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 232 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667453.0463848})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 233 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667454.058468})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 234 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667455.004188})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 235 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667455.9594843})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 236 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667456.9671373})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 237 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667457.972928})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 238 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667458.9196107})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 239 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667459.8677385})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 240 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667460.8932245})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 241 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667461.9003253})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 242 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667462.8368165})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 243 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667463.7879663})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 244 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667464.7700922})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 245 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667465.7978957})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 246 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667466.7371962})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 247 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667467.7118576})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 248 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667468.69322})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 249 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667469.7245877})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 250 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667470.6588264})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 251 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667471.618646})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 252 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667472.6014245})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 253 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667473.6182756})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 254 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667474.5637045})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 255 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667475.5113065})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 256 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667476.493722})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 257 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667477.5181382})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 258 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667478.4644759})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 259 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667479.4411037})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 260 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667480.4217865})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 261 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667481.4534452})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 262 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667482.3858361})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 263 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667483.356346})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 264 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667484.3378184})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"(User provided step: 265 is less than current step: 1232. Dropping entry: {'loss': nan, '_timestamp': 1720667485.3445904})."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:x1jfgevm). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112598299991481, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b85ce752b074357a083ced52ede889b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240711_031144-fdph3dk0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/srikarv44/LangGPT/runs/fdph3dk0' target=\"_blank\">woven-wood-22</a></strong> to <a href='https://wandb.ai/srikarv44/LangGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/srikarv44/LangGPT' target=\"_blank\">https://wandb.ai/srikarv44/LangGPT</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/srikarv44/LangGPT/runs/fdph3dk0' target=\"_blank\">https://wandb.ai/srikarv44/LangGPT/runs/fdph3dk0</a>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/srikarv44/LangGPT/runs/fdph3dk0?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7f1537e35510>"},"metadata":{}}]},{"cell_type":"code","source":"# :: DATASET ::\n# Download dataset from Hugging-face: https://huggingface.co/datasets/cfilt/iitb-english-hindi\nprint(\"INFO: Dataset download started.\")\nraw_train_dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=\"train\")\nraw_val_dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=\"validation\")\nraw_test_dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=\"test\")\nprint(\"INFO: Dataset download complete.\")\n\n\n# # Splitting the dataset into training and validation dataset of 3000 and 300 respectively for faster training and validation.\nraw_train_dataset, rt_to_skip = random_split(raw_train_dataset, [500000, len(raw_train_dataset) - 500000])\nprint(len(raw_train_dataset))\n# raw_val_dataset, vt_to_skip = random_split(raw_val_dataset, [500, len(raw_val_dataset) - 500])","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:28:33.824982Z","iopub.execute_input":"2024-07-11T03:28:33.825674Z","iopub.status.idle":"2024-07-11T03:28:37.881398Z","shell.execute_reply.started":"2024-07-11T03:28:33.825643Z","shell.execute_reply":"2024-07-11T03:28:37.880431Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"INFO: Dataset download started.\nINFO: Dataset download complete.\n500000\n","output_type":"stream"}]},{"cell_type":"code","source":"# :: TOKENIZER :: \n# [ Creating Source Language Tokenizer - English ]\n# Additional Special Tokens: [UNK] - to represent Unknown words, [PAD] - to represent padding added to keep sequence length constant for the model\n# [CLS] - Token to denote start of sentence, [SEP] = Token to denote end of sentence\n\ntokenizer_en = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer_en = BpeTrainer(\n    min_frequency=2, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n)\n\n# NOTE: below function is used as an iterator on the smaller random dataset we just \ndef get_ds_iterator(raw_train_dataset, lang):\n  for data in raw_train_dataset:\n    yield data['translation'][lang]\n    \n\n# splitting tokens based on whitespaces\ntokenizer_en.pre_tokenizer = Whitespace()\nprint(\"INFO: source tokenizer initialized\")\n\nprint(\"INFO: source tokenizer training started...\")\nstart_time = time.time()\ntokenizer_en.train_from_iterator(get_ds_iterator(raw_train_dataset, \"en\"), trainer=trainer_en)\n# tokenizer_en.train(files=path_en, trainer=trainer_en)\nprint(\"INFO: source tokenizer training completed!\")\nprint(f\"INFO: time taken: {time.time() - start_time}s\")\n\n\n# Save tokenizer for future use\ntokenizer_en.save(\"/kaggle/working/tokenizer_en.json\")\nprint(\n    f\"INFO: source tokenizer saved into: /models/tokenizer_en/tokenizer_en.json\"\n)\n\n\n# [ Creating Target Language Tokenizer - Hindi ]\ntokenizer_hi = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer_hi = BpeTrainer(\n    min_frequency=2, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n)\n\n# splitting tokens based on whitespaces\ntokenizer_hi.pre_tokenizer = Whitespace()\nprint(\"INFO: target tokenizer initialized\")\n\nprint(\"INFO: target tokenizer training started...\")\nstart_time = time.time()\ntokenizer_hi.train_from_iterator(get_ds_iterator(raw_train_dataset, \"hi\"), trainer=trainer_hi)\n# tokenizer_hi.train(files=path_hi, trainer=trainer_hi)\nprint(\"INFO: target tokenizer training completed!\")\nprint(f\"INFO: time taken: {time.time() - start_time}s\")\n\n# Save tokenizer for future use\ntokenizer_hi.save(\"/kaggle/working/tokenizer_hi.json\")\nprint(\n    f\"INFO: source tokenizer saved into: /models/tokenizer_hi/tokenizer_hi.json\"\n)\n\n# Load tokenizers from file\ntokenizer_en = Tokenizer.from_file(\"/kaggle/working/tokenizer_en.json\")\ntokenizer_hi = Tokenizer.from_file(\"/kaggle/working/tokenizer_hi.json\")\n\n# Store the vocab size of source and target tokenizers\nsource_vocab_size = tokenizer_en.get_vocab_size()\ntarget_vocab_size = tokenizer_hi.get_vocab_size()\nprint(f\"INFO: source tokenizer vocab size = {source_vocab_size}\")\nprint(f\"INFO: target tokenizer vocab size = {target_vocab_size}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:28:42.378546Z","iopub.execute_input":"2024-07-11T03:28:42.378903Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"INFO: source tokenizer initialized\nINFO: source tokenizer training started...\n","output_type":"stream"}]},{"cell_type":"code","source":"# :: TRAINING ::\nst_time = time.time()\n\n# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"INFO: device = {device}\")\n\nprint(\"INFO: line:23 -> EncodeDataset\")\n# This class takes raw dataset and max_seq_len\nclass EncodeDataset(Dataset):\n    def __init__(self, raw_dataset, max_seq_len):\n        \"\"\"\n        Constructor to initialise class variables\n\n        Args:\n            raw_dataset (Dataset): raw data downloaded from hugging-face\n            max_seq_len (int): max seq length of the sentences in the dataset\n        \"\"\"\n        super().__init__()\n        self.raw_dataset = raw_dataset\n        self.max_seq_len = max_seq_len\n\n    def __len__(self):\n        return len(self.raw_dataset)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Generating below for the translation pair at raw_dataset[index]:\n        - encoder_input\n        - decoder_input\n        - target_label\n        - encoder_mask\n        - decoder_mask\n        - source_text\n        - target_text\n\n        Args:\n            index (_type_): item at index in raw_dataset\n        \"\"\"\n\n        # Fetch the raw translation\n        raw_text = self.raw_dataset[index]\n\n        # Split into source and target text\n        source_text = raw_text['translation']['en']\n        target_text = raw_text['translation']['hi']\n\n        # Encoding source text using source tokenizer(tokenizer_en) and target text using target tokenizer(tokenizer_hi)\n        source_text_encoded = tokenizer_en.encode(source_text).ids\n        target_text_encoded = tokenizer_hi.encode(target_text).ids\n\n        # Convert the CLS, SEP and PAD tokens to their corresponding index id in vocabulary using tokenizer [the id would be same with either tokenizers]\n        CLS_ID = torch.tensor([tokenizer_hi.token_to_id(\"[CLS]\")], dtype=torch.int64)\n        SEP_ID = torch.tensor([tokenizer_hi.token_to_id(\"[SEP]\")], dtype=torch.int64)\n        PAD_ID = torch.tensor([tokenizer_hi.token_to_id(\"[PAD]\")], dtype=torch.int64)\n        \n        # To train the model we have to same sequence length for input and output and hence we need to add padding\n        # Calculate the number of padding to be added for source and target\n        num_source_padding = (\n            self.max_seq_len - len(source_text_encoded) - 2\n        )  # 2 -> [CLS] and [SEP]\n        num_target_padding = (\n            self.max_seq_len - len(target_text_encoded) - 1\n        )  # 1 -> [SEP] only because target label contains [SEP] only and [CLS] is required by the model to start the inference\n\n        # Add the padding based on the number computer above\n        encoder_padding = torch.tensor([PAD_ID] * num_source_padding, dtype=torch.int64)\n        decoder_padding = torch.tensor([PAD_ID] * num_target_padding, dtype=torch.int64)\n\n        # construct the encoder input\n        # Encoder I/P: [CLS_ID] + source_text_encoded + [SEP_ID] + encoder_padding\n        encoder_input = torch.cat([CLS_ID, torch.tensor(source_text_encoded, dtype=torch.int64), SEP_ID, encoder_padding], dim=0)\n\n        # construct the decoder input\n        # Decoder I/P: [CLS_ID] + target_text_encoded + decoder_padding\n        decoder_input = torch.cat([CLS_ID, torch.tensor(target_text_encoded, dtype=torch.int64), decoder_padding ], dim=0)\n\n        # construct the target label\n        # Target Label: target_text_encoded + [SEP_ID] + decoder_padding\n        target_label = torch.cat([torch.tensor(target_text_encoded, dtype=torch.int64), SEP_ID, decoder_padding], dim=0)\n\n        # As we are adding extra padding to match the sequence input,but we should not let the model train on it\n        # hence, we'll use encoder mask to nullify the padding tokens\n        encoder_mask = (\n            (encoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int()\n        )\n\n        # We'll do that same for decoder too but we also need to get rid of the upper triangle for Masked Multi-Attention\n        decoder_mask = (decoder_input != PAD_ID).unsqueeze(0).unsqueeze(\n            0\n        ).int() & causal_mask(decoder_input.size(0))\n\n        return {\n            \"encoder_input\": encoder_input,\n            \"decoder_input\": decoder_input,\n            \"target_label\": target_label,\n            \"encoder_mask\": encoder_mask,\n            \"decoder_mask\": decoder_mask,\n            \"source_text\": source_text,\n            \"target_text\": target_text,\n        }\n\n\n# Causal mask will make sure any token that comes after the current token will be masked, meaning the value will be replaced by -ve infinity which will be converted to zero or close to zero after softmax function.\n# Hence the model will just ignore these value or willn't be able to learn anything from these values.\ndef causal_mask(size):\n    # dimension of causal mask (batch_size, seq_len, seq_len)\n    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n    return mask == 0\n\n\n# calculating max_seq_len from the dataset\nmax_seq_len_source = 0\nmax_seq_len_target = 0\n\nfor data in raw_train_dataset:\n    enc_ids = tokenizer_en.encode(data['translation']['en']).ids\n    dec_ids = tokenizer_hi.encode(data['translation']['hi']).ids\n    max_seq_len_source = max(max_seq_len_source, len(enc_ids))\n    max_seq_len_target = max(max_seq_len_target, len(dec_ids))\n\n\nprint(f\"Max sequence length of source: {max_seq_len_source}\")  # 50\nprint(f\"Max sequence length of target: {max_seq_len_target}\")  # 50\n\n\n# To simplify the calcualtion let's add some value to the greater value and have a single max_seq_len\n#max_seq_len = 100  # 50 + 20: 20 -> to accomodate the additional length of tokens such as PAD, CLS, SEP in the sequence.\nmax_seq_len = max(max_seq_len_source, max_seq_len_target) + 20\n\n\nprint(\"INFO: Encoding dataset started.\")\n# Instantiate the EncodeDataset class and create the encoded train and validation-dataset.\ntrain_dataset = EncodeDataset(raw_train_dataset, max_seq_len)\nval_dataset = EncodeDataset(raw_val_dataset, max_seq_len)\nprint(\"INFO: Encoding dataset complete.\")\n\n\n# --------------------------------------------------- #\n# Input Embedding and Positional Encoding\n\nprint(\"INFO: line 152 -> EmbeddingLayer\")\n\n\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int):\n        super().__init__()\n        self.d_model = d_model\n\n        # using pytorch's embedding module we will map the token_id with the vocabulary and then convert it to embedding matrix\n        self.embedding = nn.Embedding(\n            vocab_size, d_model\n        )  # initialise the Embedding layer to taken in the vocab_size and output a embeddign vector of size d_model\n\n    def forward(self, input):\n        # After the output of embedding is recieved the output is multiplied with teh sqrt(d_model) for normalizing the output\n        embedding_output = self.embedding(input) * math.sqrt(self.d_model)\n        return embedding_output\n    \n\nprint(\"INFO: line 169 -> PositionalEncoding\")\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_seq_len: int, dropout_rate: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # we're creating a zero matrix of the same size as the embedding matrix\n        pe = torch.zeros(max_seq_len, d_model)\n\n        # Calculate the position part of the PE function\n        pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n\n        # Calculate the division part of the PE function\n        # NOTE: div part expression is slightly different that papers expression as this exponential functions seems to works better.\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\n        # Fill in the `pe` with the sin and cos of the PE function\n        # NOTE: sin -> even pos\n        # NOTE: cos -> odd pos\n\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n\n        # Since we're expecting the input sequences in batches so the extra batch_size dimension is added in 0 postion.\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe) # LEARN: what is register_buffer\n\n    def forward(self, input_embdding):\n        # Add positional encoding together with the input embedding vector.\n        input_embdding = input_embdding + (\n            self.pe[:, : input_embdding.shape[1], :]\n        ).requires_grad_(False) # to prevent from calculating the gradient of the positional encoding.\n\n        # Perform dropout to prevent overfitting.\n        return self.dropout(input_embdding)\n\n    \n# --------------------------------------------------- #\n# Multi-head Attention Block\n\nprint(\"INFO: line 208 -> MultiHeadAttention\")\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout_rate: float):\n        super().__init__()\n        # Define dropout to prevent overfitting\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # Weight matrices are intoduced and all are learnable params\n        self.W_q = nn.Linear(d_model, d_model, bias=False)  # Linear -> to enable learning # NOTE: bias=False -> to prevent bias newly added\n        self.W_k = nn.Linear(d_model, d_model, bias=False)\n        self.W_v = nn.Linear(d_model, d_model, bias=False)\n        self.W_o = nn.Linear(d_model, d_model, bias=False)\n\n        self.num_heads = num_heads\n        assert d_model % num_heads == 0, \"d_model must be divisible by number of heads\"\n\n        # d_k is the new dimension of each of each splitted self-attention heads\n        self.d_k = d_model // num_heads\n\n    def forward(self, q, k, v, encoder_mask):\n\n        # # We'll be training our model with multiple batches of sequence at once in parallel, hence we'll need to include batch_size in the shape as well.\n        # query, key and value are calculated by matrix multiplication of corresponding weights with the input embeddings.\n        # Change of shape: q(batch_size, seq_len, d_model) @ W_q(d_model, d_model) => query(batch_size, seq_len, d_model) [same goes to key and value].\n        query = self.W_q(q)\n        key = self.W_k(k)\n        value = self.W_v(v)\n\n        # Splitting query, key and value into number of heads. d_model is splitted in d_k across 8 heads.\n        # Change of shape: query(batch_size, seq_len, d_model) => query(batch_size, seq_len, num_heads, d_k) -> query(batch_size,num_heads, seq_len,d_k) [same goes to key and value].\n        query = query.view(query.shape[0], query.shape[1], self.num_heads ,self.d_k).transpose(1,2)\n        key = key.view(key.shape[0], key.shape[1], self.num_heads ,self.d_k).transpose(1,2)\n        value = value.view(value.shape[0], value.shape[1], self.num_heads ,self.d_k).transpose(1,2)\n\n        # INFO: SELF-ATTENTION BLOCK STARTS INFO:\n        \n        with autocast():\n            attention_score = (query @ key.transpose(-2, -1)) / math.sqrt(self.d_k)\n            attention_score = attention_score.float()\n            \n            if encoder_mask is not None:\n                attention_score = attention_score.masked_fill(encoder_mask == 0, -1e9)\n\n            attention_score = F.softmax(attention_score, dim=-1)\n            if self.dropout is not None:\n                attention_score = self.dropout(attention_score)\n\n            attention_output = attention_score @ value\n\n#         # Attention score is calculated\n#         # Change of shape: query(batch_size,num_heads, seq_len,d_k) @ key(batch_size,num_heads, seq_len,d_k) => attention_score(batch_size,num_heads, seq_len,seq_len).\n#         attention_score = (query @ key.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n#         # Cast to float32 before applying the mask and softmax\n#         attention_score = attention_score.float()\n        \n#         # If masking is available\n#         if encoder_mask is not None:\n#             attention_score.masked_fill_(encoder_mask==0, -1e9)\n\n#         # Softmax function calculates the probability distribution among all the attention scores. It assign higher probabiliy value to higher attention score. Meaning more similar tokens get higher probability value.\n#         # Change of shape: same as attention_score\n#         # attention_weight = torch.softmax(attention_score, dim=-1)\n#         attention_score = attention_score.softmax(dim=-1)\n\n#         if self.dropout is not None:\n#             attention_score = self.dropout(attention_score)\n        \n#         # Cast back to the original dtype if necessary\n#         #attention_score = attention_score.type_as(query)\n\n#         # Final step in Self attention block is, matrix multiplication of attention_weight with Value embedding vector.\n#         # Change of shape: attention_score(batch_size,num_heads, seq_len,seq_len) @  value(batch_size,num_heads, seq_len,d_k) => attention_output(batch_size,num_heads, seq_len,d_k)\n#         attention_output = attention_score @ value\n\n        # INFO: SELF-ATTENTION BLOCK ENDS\n\n        # Now, all heads must be combined back to a single head\n        # Change of shape:attention_output(batch_size,num_heads, seq_len,d_k) => attention_output(batch_size,seq_len,num_heads,d_k) => attention_output(batch_size,seq_len,d_model)\n        attention_output = attention_output.transpose(1,2).contiguous().view(attention_output.shape[0], -1, self.num_heads * self.d_k)\n\n        # Finally attention_output is matrix multiplied with output weight matrix to give the final Multi-Head attention output.\n        # The shape of the multihead_output is same as the embedding input\n        # Change of shape: attention_output(batch_size,seq_len,d_model) @ W_o(d_model, d_model) => multihead_output(batch_size, seq_len, d_model)\n        multihead_output = self.W_o(attention_output)\n\n        return multihead_output\n\n    \n# --------------------------------------------------- #\n# Feed Forward, Layer Norm and Add & Norm\n\nprint(\"INFO: line 282 -> FeedForward\")\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout_rate: float):\n        super().__init__()\n\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_1 = nn.Linear(d_model, d_ff)\n        # self.activation_1 = nn.ReLU()\n        self.layer_2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, input):\n        # return self.layer_2(self.dropout(self.activation_1(self.layer_1(input))))\n        return self.layer_2(self.dropout(torch.relu(self.layer_1(input))))  # NOTE: relu is used instead of activation_1\n\n    \nprint(\"INFO: line 298 -> LayerNorm\")\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, eps: float = 1e-5):\n        super().__init__()\n        # Epselon helps prevent potential division by 0\n        self.eps = eps\n\n        # Extra learning parameters gamma and beta are introduced to scale and shift the embedding value as the network needed.\n        self.gamma = nn.Parameter(torch.ones(512)) # 512 is advisable to be the same as d_model\n        self.beta = nn.Parameter(torch.zeros(512))\n\n    def forward(self, input):\n        mean = input.mean(dim=-1, keepdim=True)\n        std = input.std(dim=-1, keepdim=True)\n\n        return self.gamma * (input - mean)/(std + self.eps) + self.beta\n\n\nprint(\"INFO: line 317 -> AddAndNorm\")\n\n\nclass AddAndNorm(nn.Module):\n    def __init__(self, dropout_rate: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_norm = LayerNorm()\n\n    def forward(self, input, sub_layer):\n        return input + self.dropout(sub_layer(self.layer_norm(input)))\n    \n    \n# --------------------------------------------------- #\n# Encode block and Encoder\n\nprint(\"INFO: line 333 -> EncoderBlock\")\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float) -> None:\n        super().__init__()\n        self.multihead_attention = multihead_attention\n        self.feed_forward = feed_forward\n        # self.add_and_norm_list = nn.ModuleList(\n        #     [AddAndNorm(dropout_rate) for _ in range(2)]\n        # )  # 2 Add & Norm layers for every Encoder Block\n        self.addnorm_1 = AddAndNorm(dropout_rate)\n        self.addnorm_2 = AddAndNorm(dropout_rate)\n\n    def forward(self, encoder_input, encoder_mask):\n        # First AddAndNorm unit taking encoder input from skip connection and adding it with the output of MultiHead attention block\n        encoder_input = self.addnorm_1(encoder_input, lambda encoder_input: self.multihead_attention(encoder_input, encoder_input, encoder_input, encoder_mask))\n        # Second AddAndNorm unit taking output of MultiHead attention block from skip connection and adding it with the output of Feedforward layer\n        encoder_input = self.addnorm_2(encoder_input, self.feed_forward)\n        \n        return encoder_input\n    \n    \nprint(\"INFO: line 353 -> Encoder\")\n\n\nclass Encoder(nn.Module):\n    def __init__(self, encoderblocklist: nn.ModuleList):\n        super().__init__()\n\n        self.encoderblocklist = encoderblocklist\n        self.layer_norm = LayerNorm()\n\n    def forward(self, encoder_input, encoder_mask):\n        # loop through the encoderblocklist - 6 blocks\n        for encoderblock in self.encoderblocklist:\n            encoder_input = encoderblock(encoder_input, encoder_mask)\n\n        # Normalize the final encoder block output and return. This encoder output will be used later on as key and value for the cross attention in decoder block.\n        encoder_output = self.layer_norm(encoder_input)\n\n        return encoder_output\n    \n\n# --------------------------------------------------- #\n# Decoder block, Decoder and Projection\n\nprint(\"INFO: line 378 -> DecoderBlock\")\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, masked_multihead_attention: MultiHeadAttention, cross_multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float) -> None:\n        super().__init__()\n        self.masked_multihead_attention = masked_multihead_attention\n        self.cross_multihead_attention = cross_multihead_attention\n        self.feed_forward = feed_forward\n        # self.add_and_norm_list = nn.ModuleList(\n        #     [AddAndNorm(dropout_rate) for _ in range(3)]\n        # )\n        self.addnorm_1 = AddAndNorm(dropout_rate)\n        self.addnorm_2 = AddAndNorm(dropout_rate)\n        self.addnorm_3 = AddAndNorm(dropout_rate)\n\n    def forward(self, decoder_input, encoder_output, encoder_mask, decoder_mask):\n        # First AddAndNorm unit taking decoder input from skip connection and adding it with the output of Masked Multi-Head attention block\n        decoder_input = self.addnorm_1(decoder_input, lambda decoder_input: self.masked_multihead_attention(decoder_input, decoder_input, decoder_input, decoder_mask))\n        # Second AddAndNorm unit taking output of Masked Multi-Head attention block from skip connection and adding it with the output of MultiHead attention block\n        decoder_input = self.addnorm_2(decoder_input, lambda decoder_input: self.cross_multihead_attention(decoder_input, encoder_output, encoder_output, encoder_mask))\n        # Third AddAndNorm unit taking output of MultiHead attention block from skip connection and adding it with the output of Feedforward layer\n        decoder_input = self.addnorm_3(decoder_input, self.feed_forward)\n\n        return decoder_input\n    \n    \nprint(\"INFO: line 407 -> Decoder\")\n\n\nclass Decoder(nn.Module):\n    def __init__(self, decoderblocklist: nn.ModuleList):\n        super().__init__()\n\n        self.decoderblocklist = decoderblocklist\n        self.layer_norm = LayerNorm()\n\n    def forward(self, decoder_input, encoder_output, encoder_mask, decoder_mask):\n        for decoderblock in self.decoderblocklist:\n            decoder_input = decoderblock(decoder_input, encoder_output, encoder_mask, decoder_mask)\n\n        decoder_output = self.layer_norm(decoder_input)\n\n        return decoder_output\n    \n    \nprint(\"INFO: line 425 -> ProjectionLayer\")\n\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, d_model, vocab_size):\n        super().__init__()\n        self.projection_layer = nn.Linear(d_model, vocab_size)\n\n    def forward(self, decoder_output):\n        # Projection layer first take in decoder output and passed into the linear layer of shape (d_model, vocab_size)\n        # Change in shape: decoder_output(batch_size, seq_len, d_model) @ linear_layer(d_model, vocab_size) => output(batch_size, seq_len, vocab_size)\n        output = self.projection_layer(decoder_output)\n\n        # softmax function to output the probability distribution over the vocabulary\n        # return torch.log_softmax(output, dim=-1)\n        return output\n\n    \n    \n# --------------------------------------------------- #\n# Transformer\n\nprint(\"INFO: line 445 -> Transformer\")\n\n\nclass Transformer(nn.Module):\n    def __init__(self,\n                 encoder: Encoder, \n                 decoder: Decoder, \n                 source_embed: EmbeddingLayer, \n                 target_embed: EmbeddingLayer, \n                 source_pos: PositionalEncoding, \n                 target_pos: PositionalEncoding, \n                 projection_layer: ProjectionLayer\n    ) -> None:\n        super().__init__()\n\n        self.source_embed = source_embed\n        self.source_pos = source_pos\n        self.encoder = encoder\n\n        self.target_embed = target_embed\n        self.target_pos = target_pos\n        self.decoder = decoder\n\n        self.projection_layer = projection_layer\n\n    # Encode function takes in encoder input, does necessary processing inside all encoder blocks and gives encoder output.\n    def encode(self, encoder_input, encoder_mask):\n        encoder_input = self.source_embed(encoder_input)\n        encoder_input = self.source_pos(encoder_input)\n        encoder_output = self.encoder(encoder_input, encoder_mask)\n        return encoder_output\n\n    # Decode function takes in decoder input, does necessary processing inside all decoder blocks and gives decoder output.\n    def decode(self, encoder_output, encoder_mask, decoder_input, decoder_mask):\n        decoder_input = self.target_embed(decoder_input)\n        decoder_input = self.target_pos(decoder_input)\n        decoder_output = self.decoder(decoder_input, encoder_output, encoder_mask, decoder_mask)\n        return decoder_output\n\n    # Projec function takes in decoder output into its projection layer and maps the output to the vocabulary for prediction.\n    def project(self, decoder_output):\n        return self.projection_layer(decoder_output)\n    \n    \n# INFO: BUILD MODEL BLOCK INFO:\n\nprint(\"INFO: line 497 -> Model build started.\")\n\n\ndef build_model(\n    source_vocab_size: int, \n    target_vocab_size: int, \n    source_seq_len: int, \n    target_seq_len: int, \n    d_model: int=512, \n    num_blocks: int=6, \n    num_heads: int=8, \n    dropout_rate: float=0.1, \n    d_ff: int=2048\n) -> Transformer:\n    # Design and assign all the values that are needed by the transformer architecture\n    source_embed = EmbeddingLayer(d_model, source_vocab_size)\n    target_embed = EmbeddingLayer(d_model, target_vocab_size)\n    \n    # Create the positional encoding layers\n    source_pos = PositionalEncoding(d_model, source_seq_len, dropout_rate)\n    target_pos = PositionalEncoding(d_model, target_seq_len, dropout_rate)\n    \n    # Create the encoder-block-list\n    encoderblocklist = []\n    for _ in range(num_blocks):\n        multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n        feed_forward = FeedForward(d_model, d_ff, dropout_rate)\n        encoder_block = EncoderBlock(multihead_attention, feed_forward, dropout_rate)\n        encoderblocklist.append(encoder_block)\n    # Create the encoder\n    encoder = Encoder(nn.ModuleList(encoderblocklist))\n\n    # Create the decoder-block-list\n    decoderblocklist = []\n    for _ in range(num_blocks):\n        masked_multihead_attention = MultiHeadAttention(d_model,num_heads, dropout_rate)\n        cross_multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n        feed_forward = FeedForward(d_model, d_ff, dropout_rate)\n        decoder_block = DecoderBlock(masked_multihead_attention, cross_multihead_attention, feed_forward, dropout_rate)\n        decoderblocklist.append(decoder_block)\n    # Create the decoder\n    decoder = Decoder(nn.ModuleList(decoderblocklist))\n\n    # Create the projection layer\n    projection_layer = ProjectionLayer(d_model, target_vocab_size)\n\n    # Now that we've initialized all the required blocks of transformer, we can now inititiate a model\n    model = Transformer(\n        encoder, \n        decoder, \n        source_embed, \n        target_embed, \n        source_pos, \n        target_pos, \n        projection_layer\n    )\n\n    for param in model.parameters():\n        if param.dim() > 1:\n            nn.init.xavier_uniform_(param)\n\n    return model\n\n\n\n# Finally, call build model and assign it to model variable.\n# This model is now fully ready to train and validate our dataset.\n# After training and validation, we can perform new translation task using this very model\n\n# Let's build the the final model.\nmodel = build_model(\n    tokenizer_en.get_vocab_size(), \n    tokenizer_hi.get_vocab_size(),\n    max_seq_len, max_seq_len, \n    d_model=512\n).to(device)\n\n# Let's look at the architecture that we've just build ourself\nprint(model)\nwandb.watch(model)\n\n# INFO: END BUILD MODEL BLOCK INFO:\nprint(\"INFO: line 562 -> Model build completed.\")\n\n\n# INFO: TRAIN MODEL BLOCK INFO:\n\n\ndef run_validation(model, validation_ds, tokenizer_en, tokenizer_hi, max_seq_len, device, print_msg, global_step):\n    model.eval()\n    count = 0\n\n    with torch.no_grad():\n        for batch in validation_ds:\n            count += 1\n            encoder_input = batch[\"encoder_input\"].to(device)\n            encoder_mask = batch[\"encoder_mask\"].to(device)\n\n            cls_id = tokenizer_hi.token_to_id('[CLS]')\n            sep_id = tokenizer_hi.token_to_id('[SEP]')\n\n            # Computing the output of the encoder for the source sequence\n            encoder_output = model.module.encode(encoder_input, encoder_mask)\n            # for prediction task, the first token that goes in decoder input is the [CLS] token\n            decoder_input = torch.empty(1, 1).fill_(cls_id).type_as(encoder_input).to(device)\n            # since we need to keep adding the output back to the input until the [SEP] - end token is received.\n            while True:\n                # check if the max length is received\n                if decoder_input.size(1) == max_seq_len:\n                    break\n\n                # recreate mask each time the new output is added the decoder input for next token prediction\n                decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n\n                # apply projection only to the next token\n                out = model.module.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n\n                # apply projection only to the next token\n                prob = model.module.project(out[:, -1])\n\n                # select the token with highest probablity which is a greedy search implementation\n                _, next_word = torch.max(prob, dim=1)\n                #print(\"next word shape: \", next_word.shape)\n                next_word = next_word.unsqueeze(0)\n#                 next_word = next_word.unsqueeze(0).transpose(0, 1).unsqueeze(0)\n                #print(\"next word shape: \", next_word.shape)\n                \n#                 decoder_input = torch.cat(\n#                     [decoder_input, torch.empty(1, 1).type_as(encoder_input).fill_(next_word.item()).to(device)], dim=1\n#                 )\n                #print(\"decoder input shape: \", decoder_input.shape)\n#                 decoder_input = torch.cat(\n#                     [decoder_input, next_word], dim=1\n#                 )\n                # Ensure decoder_input and next_word have compatible dimensions\n                decoder_input = torch.cat(\n                        [decoder_input, next_word], dim=1\n                )\n                # check if the new token is the end of token\n                if (next_word == sep_id).any():\n                    break\n            # final output is the concatinated decoder input till the end token is reached\n            model_out = decoder_input.squeeze(0)\n\n            source_text = batch[\"source_text\"][0]\n            target_text = batch[\"target_text\"][0]\n            model_out_text = tokenizer_hi.decode(model_out.detach().cpu().numpy())\n\n            # Print the source, target and model output\n            print_msg('-'*55)\n            # print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n            # print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n            # print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n            print_msg(f'Source Text: {source_text}')\n            print_msg(f'Target Text: {target_text}')\n            print_msg(f'Predicted by langGPT: {model_out_text}')\n\n            if count == 2:\n                break\n\n\n# Constants\nEPOCHS = 20\nBATCH_SIZE = 1\nACCUMULATION_STEPS = 4\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nprint(\"INFO: Dataloader started.\")\n# Creating DataLoader wrapper for both training and validation dataset. This dataloader will be used later stage during training and validation of our LLM model.\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\nprint(\"INFO: Dataloader complete.\")\n\n# Adam is one of the most commonly used optimization algorithms that hold the current state and will update the parameters based on the computed gradients.\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, eps=1e-9)\nscaler = GradScaler()\n\n# Learning Rate Scheduler\nnum_training_steps = EPOCHS * len(train_dataloader)\nlr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# The CrossEntropyLoss loss function computes the difference between the projection output and target label.\nloss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_en.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n\n# Multi-GPU\nif torch.cuda.device_count() > 1:\n    model = torch.nn.DataParallel(model)\n\n# Function to clear GPU cache\ndef clear_gpu_cache():\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n\n\ndef train_model(preload_epoch=None):\n    # The entire training, validation cycle will run for 20 cycles or epochs.\n    EPOCHS = 20\n    initial_epoch = 0\n    global_step = 0\n    epoch_loss = 0\n\n    # If the preload_epoch is not none, that means the training will start with the weights, optimizer that has been last saved and start with preload epoch + 1\n    if preload_epoch is not None:\n      model_filename = f\"/kaggle/working/model_{epoch}.pt\"\n      state = torch.load(model_filename)\n      model.load_state_dict(state['model_state_dict'])\n      initial_epoch = state['epoch'] + 1\n      optimizer.load_state_dict(state['optimizer_state_dict'])\n      global_step = state['global_step']\n\n    for epoch in range(initial_epoch, EPOCHS):\n        # torch.cuda.empty_cache()\n        model.train()\n        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n        optimizer.zero_grad()\n        \n        for batch in batch_iterator:\n            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n            target_label = batch['target_label'].to(device) # (B, seq_len)\n            \n            with autocast():\n                # Run the tensors through the encoder, decoder and the projection layer\n                encoder_output = model.module.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n                decoder_output = model.module.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n                projection_output = model.module.project(decoder_output) # (B, seq_len, vocab_size)\n\n                # Compute the loss using a simple cross entropy\n                loss = loss_fn(projection_output.view(-1, tokenizer_hi.get_vocab_size()), target_label.view(-1))\n                \n                \n            scaler.scale(loss).backward()\n            \n            if (global_step + 1) % ACCUMULATION_STEPS == 0:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad(set_to_none=True)\n                lr_scheduler.step()\n                \n            \n            global_step += 1\n            \n            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n            epoch_loss = loss.item()\n            wandb.log({\"loss\": loss.item()}, step=global_step)\n            clear_gpu_cache()\n\n#             # Backpropagate the loss\n#             loss.backward()\n\n#             # Update the weights\n#             optimizer.step()\n#             optimizer.zero_grad(set_to_none=True)\n\n            \n        \n        wandb.log({\"epoch loss\": epoch_loss}, step=epoch)\n        # VALIDATION BLOCK STARTS HERE [Runs every epoch after the training block is complete]\n        run_validation(model, val_dataloader, tokenizer_en, tokenizer_hi, max_seq_len, device, lambda msg: batch_iterator.write(msg), global_step)\n\n        # Save the model at the end of every epoch\n        model_filename = f\"/kaggle/working/model_{epoch}.pt\"\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step\n        }, model_filename)\n\n# Train our model\n\n# This function runs the training and validation for 10 epochs\ntrain_model(preload_epoch=None)\nprint(\"INFO: Model training completed.\")\nprint(f\"INFO: Time: {time.time()}\")\n\nprint(\n    f\"INFO: Total time taken(including loading dataset, training tokenizer, building the model, validating the model): {time.time() - st_time}s\"\n)\n\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}